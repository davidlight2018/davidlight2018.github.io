<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>『阅读笔记』Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs inWeb Applications</title>
      <link href="/2020/04/26/www18_donut/"/>
      <url>/2020/04/26/www18_donut/</url>
      
        <content type="html"><![CDATA[<p>这篇 paper 提出了一种基于变分自编码器（Variational Auto-Encoder, VAE）的无监督异常检测算法 <em>Donut</em>，对于来自一家顶级互联网公司的 KPI 数据可以获得 0.75~0.9 的最佳 F1 得分。</p><h2 id="背景介绍">背景介绍</h2><p>关键性能指标（Key Performance Indicators, KPI）是时间序列数据，用于测量诸如页面浏览量、用户在线数量、订单数量等等。在这些 KPI 中，大多数是与业务相关的 KPI（也是本文关注的重点）。这些 KPI 受用户行为和日程安排的影响很大，因此大致有规律的季节性模式出现（例如每天/每周）。在每个重复周期中，KPI 曲线的形状并不会完全相同，因为用户行为可能随着时间变化。这种 KPI 模式可以被称作「具有局部差异的季节性 KPI」。另一类局部变化是随着时间增加的趋势，可以通过 Holt-Winters 和 时间序列分解（Time Series Decomposition）来确定。除非正确处理了这些局部变化，否则异常检测算法可能无法正常工作。</p><p>除了 KPI 形状的季节性变化和局部变化外，这些 KPI 上还存在噪声。我们认为在每个点上它们都是独立的零均值高斯。高斯噪声的精确值是没有意义的，因此我们仅关注这些噪声的统计数据，即噪声的方差。因此我们可以将季节性 KPI 的正常模式形式化为两种组合：(1) 具有局部变化的季节性模式；(2) 高斯噪声的统计数据。</p><p>我们使用「anomalies」来表示不遵循正常模式的纪录点（如突然的尖峰或骤降），而使用「abnormal」来同时表示「anomalies」和缺失点。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-26_23-02-05.png" style="zoom:40%;" /></p><p>可以将 KPI 异常检测的公式定义如下：对于任何时间 <span class="math inline">\(t\)</span>，给定历史观测值 <span class="math inline">\(x_{t-T+1}, \cdots, x_t\)</span>，确定该时间是否发生异常（以 <span class="math inline">\(y_t=1\)</span> 表示）。异常检测算法通常计算 <span class="math inline">\(y_t=1\)</span> 的概率值 <span class="math inline">\(p(y_t=1|x_{t-T+ 1, \cdots, x_t})\)</span>，而不是直接计算 <span class="math inline">\(y_t\)</span>。之后，操作员可以通过选择阈值来决定是否声明异常，即得分超过该阈值的数据点表示异常。</p><h2 id="变分自编码器简介">变分自编码器简介</h2><p>深度贝叶斯网路使用神经网络来表达变量之间的关系，因此它们不再局限于简单的分布簇，可以轻松地用于复杂的数据。在训练和预测中经常采用变分推理技术（Variational inference techniques），这是解决由神经网络得出的后验分布的有效方法。</p><p>VAE 是一个深层的贝叶斯网络，它对两个随机变量（潜在变量 <span class="math inline">\(z\)</span> 和可见变量 <span class="math inline">\(x\)</span>）之间的关系进行建模。首先为 <span class="math inline">\(z\)</span> 选择一个先验，通常是多元的正态高斯分布 <span class="math inline">\(\mathcal{N}(0,I)\)</span>。之后，从 <span class="math inline">\(p_{\theta} (x|z)\)</span> 中采样 <span class="math inline">\(x\)</span>，其中 <span class="math inline">\(p_{\theta} (x|z)\)</span> 是从参数 <span class="math inline">\(\theta\)</span> 的神经网络中得出的。<span class="math inline">\(p_{\theta} (x|z)\)</span> 的确切形式需要根据任务需求来确定。真实的后验 <span class="math inline">\(p_{\theta} (z|x)\)</span> 是解析方法难以解出的，但是对于训练而言必不可少，并且在预测中有作用。因此，变分推理技术可用于拟合另一个神经网络作为后验 <span class="math inline">\(q_{\phi} (z|x)\)</span> 的近似。该后验通常假定为 <span class="math inline">\(\mathcal{N}\left(\boldsymbol{\mu}_{\phi}(\mathbf{x}), \boldsymbol{\sigma}_{\phi}^{2}(\mathbf{x})\right)\)</span> ，其中 <span class="math inline">\(\mu_{\phi}(\mathbf{x})\)</span> 和 <span class="math inline">\({\sigma}_{\phi}(\mathbf{x})\)</span> 都是从神经网络推出的。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-27_16-48-36.png" style="zoom:40%;" /></p><p>SGVB 是一种变分推理算法，通常与 VAE 一起使用，通过最大化证据下界 <span class="math inline">\(\text{ELBO}\)</span> 来联合近似的后验模型和生成模型，如下式所述。 <span class="math display">\[\begin{aligned}\log p_{\theta}(\mathbf{x}) &amp; \geq \log p_{\theta}(\mathbf{x})-\mathrm{KL}\left[q_{\phi}(\mathbf{z} | \mathbf{x}) \| p_{\theta}(\mathbf{z} | \mathbf{x})\right] \\&amp;=\mathcal{L}(\mathbf{x}) \\&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}\left[\log p_{\theta}(\mathbf{x})+\log p_{\theta}(\mathbf{z} | \mathbf{x})-\log q_{\phi}(\mathbf{z} | \mathbf{x})\right] \\&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}\left[\log p_{\theta}(\mathbf{x}, \mathbf{z})-\log q_{\phi}(\mathbf{z} | \mathbf{x})\right] \\&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}\left[\log p_{\theta}(\mathbf{x} | \mathbf{z})+\log p_{\theta}(\mathbf{z})-\log q_{\phi}(\mathbf{z} | \mathbf{x})\right]\end{aligned}\]</span> 通常采用蒙特卡罗积分来近似上式方程中的期望，如下式。 <span class="math display">\[\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}[f(\mathbf{z})] \approx \frac{1}{L} \sum_{l=1}^{L} f\left(\mathbf{z}^{(l)}\right)\]</span> 其中，<span class="math inline">\(\mathbf{z}^{(l)}, l=1,2,\cdots, L\)</span> 是 <span class="math inline">\(q_{\phi}(\mathbf{z})\)</span> 中的采样。</p><h2 id="donut">Donut</h2><h3 id="网络结构">网络结构</h3><p>总体架构如图 3 所示，其中三个关键技术分别是改进的 ELBO、训练中插入丢失数据以及检测中的 MCMC 插补。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-27_17-07-44.png" style="zoom:40%;" /></p><p>本文研究的 KPI 假定为具有高斯噪声的时间序列。但是，VAE 不是顺序模型，因此我们在 KPI 上应用长度为 <span class="math inline">\(W\)</span> 的滑动窗口：对于每个点 <span class="math inline">\(x_t\)</span>，我们用 <span class="math inline">\(x_{t-W+1}, \cdots, x_t\)</span> 来表示 VAE 中的 <span class="math inline">\(\mathbf{x}\)</span> 向量。</p><p>Donut 的整体网络结构如图 4 所示，其中带有双线轮廓的组件为新的设计。先验 <span class="math inline">\(p_{\theta}(\mathbf{z})\)</span> 为高斯分布 <span class="math inline">\(\mathcal{N}(0,I)\)</span>，<span class="math inline">\(\mathbf{x}\)</span> 和 <span class="math inline">\(\mathbf{z}\)</span> 的后验均选择为对角线高斯 <span class="math inline">\(p_{\theta}(\mathbf{x} | \mathbf{z})=\mathcal{N}\left(\boldsymbol{\mu}_{\mathbf{x}}, \boldsymbol{\sigma}_{\mathbf{x}}^{2} \mathbf{I}\right)\)</span> 和 <span class="math inline">\(q_{\phi}(\mathbf{z} | \mathbf{x})=\mathcal{N}\left(\boldsymbol{\mu}_{\mathbf{z}}, \boldsymbol{\sigma}_{\mathbf{z}}^{2} \mathbf{I}\right)\)</span>，其中 <span class="math inline">\(\boldsymbol{\mu}_{\mathbf{x}}, \boldsymbol{\mu}_{\mathbf{z}}, \boldsymbol{\sigma}_{\mathbf{x}}, \boldsymbol{\sigma}_{\mathbf{z}}\)</span> 是每个独立高斯分量的均值和标准差。</p><p><span class="math inline">\(\mathbf{z}\)</span> 为 <span class="math inline">\(K\)</span> 维向量。 通过分离的隐藏层 <span class="math inline">\(f_{\phi}(\mathbf{x})\)</span> 和 <span class="math inline">\(f_{\theta}(\mathbf{z})\)</span> 并从 <span class="math inline">\(\mathbf{x}\)</span> 和 <span class="math inline">\(\mathbf{z}\)</span> 中提取隐藏特征，之后隐藏特征中得出 <span class="math inline">\(\mathbf{x}\)</span> 和 <span class="math inline">\(\mathbf{z}\)</span> 的高斯参数。</p><p>平均值来自线性层：<span class="math inline">\(\boldsymbol{\mu}_{\mathbf{x}}=\mathbf{W}_{\boldsymbol{\mu}_{\mathbf{x}}}^{\top} f_{\theta}(\mathbf{z})+\mathbf{b}_{\boldsymbol{\mu}_{\mathbf{x}}}\)</span> 和 <span class="math inline">\(\boldsymbol{\mu}_{\mathbf{z}}=\mathbf{W}_{\boldsymbol{\mu}_{z}}^{\top} f_{\phi}(\mathbf{x})+\mathbf{b}_{\boldsymbol{\mu}_{\mathbf{z}}}\)</span>；</p><p>标准差是从 Soft-Plus 层并加上一个非负小数 <span class="math inline">\(\epsilon\)</span> 得出：<span class="math inline">\(\sigma_{\mathrm{x}}=\operatorname{SoftPlus}\left[\mathbf{W}_{\sigma_{\mathrm{x}}}^{\top} f_{\theta}(\mathbf{z})+\mathbf{b}_{\sigma_{\mathrm{x}}}\right]+\epsilon\)</span> 以及 <span class="math inline">\(\sigma_{\mathrm{z}}=\operatorname{SoftPlus}\left[\mathbf{W}_{\sigma_{\mathrm{z}}}^{\top} f_{\theta}(\mathbf{x})+\mathbf{b}_{\sigma_{\mathrm{z}}}\right]+\epsilon\)</span>，其中 <span class="math inline">\(\operatorname{SoftPlus}[a] = \text{log}[\text{exp}(a)+1]\)</span>。所有的 <span class="math inline">\(\mathbf{W}\)</span> 和 <span class="math inline">\(\mathbf{b}\)</span> 都是相应层的参数。需要注意的是，标量函数 <span class="math inline">\(f(x)\)</span> 应用于向量 <span class="math inline">\(\mathbf{x}\)</span> 时，是应用在它的每个分量上的。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-27_17-13-23.png" style="zoom:40%;" /></p><p>我们选择以这种方法来导出 <span class="math inline">\(\boldsymbol{\sigma}_{\mathbf{x}}, \boldsymbol{\sigma}_{\mathbf{z}}\)</span>，而不是像其他人那样使用线性层来导出 <span class="math inline">\(\text{log}\boldsymbol{\sigma}_{\mathbf{x}}, \text{log}\boldsymbol{\sigma}_{\mathbf{z}}\)</span>，是因为我们感兴趣的 KPI 局部变化非常小，以至于 <span class="math inline">\(\boldsymbol{\sigma}_{\mathbf{x}}, \boldsymbol{\sigma}_{\mathbf{z}}\)</span> 可能非常接近于 0，从而使 <span class="math inline">\(\text{log}\boldsymbol{\sigma}_{\mathbf{x}}, \text{log}\boldsymbol{\sigma}_{\mathbf{z}}\)</span> 无界。而这种情况在计算高斯变量的可能性时，将导致严重的数值问题。因此，我们使用 soft-plus 和 <span class="math inline">\(\epsilon\)</span> 技巧来避免此类问题。之后使用修改的 ELBO（M-ELBO）来排除异常点和缺失点的影响。</p><h3 id="训练">训练</h3><p>训练过程很直接，通过使用 SGVB 算法来优化 ELBO。当使用 SGVB 算法训练 VAE 时，一个样本足以计算 ELBO，因此在训练期间让采样数 <span class="math inline">\(L=1\)</span>。<span class="math inline">\(\mathbf{x}\)</span> 的窗口在每个训练轮次之前随机打乱，这对随机梯度下降很有用。在每个小批量训练中要使用足够多的 <span class="math inline">\(\mathbf{x}\)</span>，这对于训练的稳定性至关重要，因为采样会引入额外的随机性。</p><p>基于 VAE 的异常检测通过学习正常模式来工作，因此我们需要尽可能避免学习到异常模式。尝试用拟合值来替换数据中的标记异常（如果有）和缺失点（已知）是一种方法，先前的一些工作<sup id="a1"><a href="#f1">1</a></sup>提出了填补缺失数据的方法，但是很难产生足够好的遵循「正常模式的数据」。更重要的是，用另一种算法生成的数据来训练生成模型是很荒谬的，因为生成模型的一个主要应用就是生成数据，使用比 VAE 更弱的算法估算的数据可能会降低性能。因此，我们不会在训练 VAE 之前填补缺失数据，而是简单地将缺失点填充为零（在图 3 的准备步骤中）。更确切地说，我们将 ELBO 的标准公式修改为如下公式： <span class="math display">\[\widetilde{\mathcal{L}}(\mathbf{x})=\mathbb{E}_{q_{\phi}(\mathrm{z} | \mathbf{x})}\left[\sum_{w=1}^{W} \alpha_{w} \log p_{\theta}\left(x_{w} | \mathbf{z}\right)+\beta \log p_{\theta}(\mathbf{z})-\log q_{\phi}(\mathbf{z} | \mathbf{x})\right]\]</span> 其中，<span class="math inline">\(\alpha_{w}\)</span> 是一个指标，<span class="math inline">\(\alpha_{w}=1\)</span> 表示 <span class="math inline">\(x_w\)</span> 没有异常或缺失，否则 <span class="math inline">\(\alpha_{w}=0\)</span>；<span class="math inline">\(\beta\)</span> 定义为 <span class="math inline">\(\left(\sum_{w=1}^{W} \alpha_{w}\right) / W\)</span>。当训练数据中没有标记的异常时，上述公式仍然成立。<span class="math inline">\(a_w\)</span> 直接排除了来自标记异常点和缺失点的 <span class="math inline">\(p_{\theta}(x_{w} | \mathbf{z})\)</span> 的贡献，而缩放因子 <span class="math inline">\(\beta\)</span> 根据 <span class="math inline">\(\mathbf{x}\)</span> 中正常点的比例来缩小 <span class="math inline">\(p_{\theta}(\mathbf{z})\)</span> 的贡献。即使 <span class="math inline">\(\mathbf{x}\)</span> 中存在某些异常点，这样的修改能够使得 Donut 正确地重建 <span class="math inline">\(\mathbf{x}\)</span> 中的正常点。我们不缩小 <span class="math inline">\(q_{\phi}(\mathbf{z} | \mathbf{x})\)</span> 基于以下两点考虑：(1) 与作为生成网络（即「正常模式」的模型）中一部分的 <span class="math inline">\(p_{\theta}(\mathbf{z})\)</span> 不同，<span class="math inline">\(q_{\phi}(\mathbf{z} | \mathbf{x})\)</span> 仅描述了 <span class="math inline">\(\mathbf{x}\)</span> 到 <span class="math inline">\(\mathbf{z}\)</span> 的映射，而不考虑「正常模式」。因此，似乎没有必要去掉 <span class="math inline">\(q_{\phi}(\mathbf{z} | \mathbf{x})\)</span> 的贡献；(2) 另一个原因是 <span class="math inline">\(\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}\left[-\log q_{\phi}(\mathbf{z} | \mathbf{x})\right]\)</span> 恰好是 <span class="math inline">\(q_{\phi}(\mathbf{z} | \mathbf{x})\)</span> 的熵，在训练中还有其他作用，因此最好保持不变。</p><p>处理异常和缺失点的另一种方法是从训练数据中排除所有包含这些点的窗口。事实证明，这种方法不如 M-ELBO。此外，我们还在训练中引入注入缺失数据的方法：我们以随机概率 <span class="math inline">\(\lambda\)</span> 将正常点设置为零，就好像它们是缺失点一样。缺失点更多，当给定异常 <span class="math inline">\(\mathbf{x}\)</span> 时，会更频繁的训练 Donut 来重建正常点，从而增强 M-ELBO 的效果。该注入会在每个训练轮次之前都执行，然后在轮次结束后将这些注入恢复。图 3 的训练阶段显示了注入缺失数据的过程。</p><h3 id="检测">检测</h3><p>诸如 VAE 的生成模型可以得出各种输出。在异常检测的范围内，观察窗口 <span class="math inline">\(\mathbf{x}\)</span> 的可能性（即 VAE 中的 <span class="math inline">\(p_{\theta}(\mathbf{x})\)</span>）是一个重要的输出，因为我们想知道给定 <span class="math inline">\(\mathbf{x}\)</span> 遵循正常模式的程度。我们可以使用蒙特卡罗方法计算 <span class="math inline">\(\mathbf{x}\)</span> 的概率密度，通过 <span class="math inline">\(p_{\theta}(\mathbf{x})=\mathbb{E}_{p_{\theta}(\mathbf{z})}\left[p_{\theta}(\mathbf{x} | \mathbf{z})\right]\)</span>。尽管在理论上有很好的解释性，但实际上，对先验样本进行采样并不能很好地完成工作。</p><p>人们可能会寻求通过计算后验 <span class="math inline">\(q_{\phi}(\mathbf{z} | \mathbf{x})\)</span> 来获得有用的输出来替代在先验上采样，其中一种选择是计算等式 <span class="math inline">\(\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}\left[p_{\theta}(\mathbf{x} | \mathbf{z})\right]\)</span>。尽管它与 <span class="math inline">\(p_{\theta}(\mathbf{x})\)</span> 相似，但它不是明确定义的概率密度。另一种选择是计算公式 ，称为<span class="math inline">\(\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}\left[\log p_{\theta}(\mathbf{x} | \mathbf{z})\right]\)</span>「重建概率」。这两个选择非常相似，由于在异常检测中，我们只关注异常评分的顺序而不是确切值，因此我们遵循第二种方法。或者，使用 ELBO 等式也可以用于近似 <span class="math inline">\(\log p_\theta(\mathbf{x})\)</span>，但是等式中额外的部分 <span class="math inline">\(\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}\left[\log p_{\theta}(\mathbf{z})-\log q_{\phi}(\mathbf{z} | \mathbf{x})\right]\)</span> 使其内部机制难以理解。而且，实验不支持该替代方案的优越性，因此我们选择不使用它。</p><p>在检测期间，测试窗口 <span class="math inline">\(\mathbf{x}\)</span> 中的异常和缺失点可能给映射 <span class="math inline">\(\mathbf{z}\)</span> 带来偏差，并进一步使得重建概率不准确。由于缺失点总是已知的，即「null」，因此我们有机会消除缺失点带来的偏差。我们选择训练好的 VAE 并使用基于 MCMC 的缺失数据插补技术。</p><p>另一方面，由于我们不知道异常在检测之前的确切位置，因此无法对异常采用 MCMC。更具体地说，我们将测试 <span class="math inline">\(\mathbf{x}\)</span> 分为观察到的部分和缺失部分连个部分，即 <span class="math inline">\((\mathbf{x}_{o}, \mathbf{x}_{m})\)</span>。从 <span class="math inline">\(q_{\phi}\left(\mathbf{z} | \mathbf{x}_{o}, \mathbf{x}_{m}\right)\)</span> 中获得样本 <span class="math inline">\(\mathbf{z}\)</span>，然后从 <span class="math inline">\(p_{\theta}\left(\mathbf{x}_{o}, \mathbf{x}_{m} | \mathbf{z}\right)\)</span> 中获得重构样本 <span class="math inline">\(\left(\mathbf{x}_{o}^{\prime}, \mathbf{x}_{m}^{\prime}\right)\)</span>。然后将 <span class="math inline">\((\mathbf{x}_{o}, \mathbf{x}_{m})\)</span> 替换为 <span class="math inline">\(\left(\mathbf{x}_{o}, \mathbf{x}_{m}^{\prime}\right)\)</span>，即观察点固定，缺失点设置为新值。将该过程重复 M 次，然后将最终值 <span class="math inline">\(\left(\mathbf{x}_{o}, \mathbf{x}_{m}^{\prime}\right)\)</span> 用于计算重建概率。在整个过程中，中间值 <span class="math inline">\(\mathbf{x}_{m}^{\prime}\)</span> 会越来越接近正常值。给定足够大的 M，可以减少偏差，并且可以获得更准确的重构概率。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-28_10-53-49.png" style="zoom:40%;" /></p><p>在 MCMC 之后，我们取 <span class="math inline">\(L\)</span> 个 <span class="math inline">\(\mathbf{z}\)</span> 样本，通过蒙特卡罗积分来计算重建概率。尽管我们可以计算 <span class="math inline">\(\mathbf{x}\)</span> 的每个窗口中的每个点的重建概率，但我们只使用最后一个点的分数（即 <span class="math inline">\(x_{t-T+1}, \cdots, x_t\)</span> 中的 <span class="math inline">\(x_t\)</span>），因为我们想要在检测过程中越快的对异常作出响应。在之后的内容中，我们仍将使用矢量符号，与 VAE 的体系结构相对应。</p><h2 id="实验评估">实验评估</h2><h3 id="数据集">数据集</h3><p>选择了 3 个数据集，如图 1 所示。数据集 A、B 和 C 分别具有小、中、大的噪声（曲线 A 最为平滑）。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-28_13-46-51.png" style="zoom:50%;" /></p><p>在我们的评估中，我们完全忽略了所有算法在缺失点（null）的输出，会对其他所有点计算一个异常分数。可以通过选择一个阈值来做出决定：如果某个点的得分大于阈值，则应该触发警报。</p><h3 id="参数设置">参数设置</h3><p>我们将滑动窗口大小 <span class="math inline">\(W\)</span> 设置为 120，在数据集中意味着 2 小时。<span class="math inline">\(W\)</span> 的选择受到两个因素的影响：(1) 太小的 <span class="math inline">\(W\)</span> 将导致模型无法捕获模式，因为模型期望从窗口中的信息中识别出正常模式；(2) 而 <span class="math inline">\(W\)</span> 太大则会增加过拟合的风险，因为我们保持完全连接层而不使用权重共享，因此模型的参数量与 <span class="math inline">\(W\)</span> 成正比。</p><p><span class="math inline">\(\mathbf{z}\)</span> 的维度，即 <span class="math inline">\(K\)</span> 的大小，起着重要的作用。<span class="math inline">\(K\)</span> 太小可能会导致拟合不足或次优平衡，另一方面，<span class="math inline">\(K\)</span> 太大可能会导致重建概率找不到好的后验概率。在完全无监督的情况下，很难选择一个好的 <span class="math inline">\(K\)</span>，因此我们将其留作未来的工作。在试验中，我们建议在 5-10 的范围内根据经验选择 <span class="math inline">\(K\)</span>。</p><p>隐藏层 <span class="math inline">\(q_{\phi}(\mathbf{z} | \mathbf{x})\)</span> 和 <span class="math inline">\(p_{\theta}(\mathbf{x} | \mathbf{z})\)</span> 都选择使用两个具有 100 个单元的 ReLU 层，使得变分网络和生成网络具有相同的大小。我们没有对隐藏网络的结构进行详细的搜索。</p><p>对于 std 层我们设置 <span class="math inline">\(\epsilon=10^{-4}\)</span>，异常插入比率 <span class="math inline">\(\lambda=0.01\)</span>，MCMC 的迭代次数 <span class="math inline">\(M=10\)</span>，蒙特卡罗的采样数 <span class="math inline">\(L=1024\)</span>。训练的批大小设置为 256，并运行 250 个轮次。使用 Adam 优化器，其初始学习率为 <span class="math inline">\(10^{-3}\)</span>。每 10 个轮次，学习率会乘以 0.75 来降低。我们对隐藏层应用 L2 正则化，其系数为 <span class="math inline">\(10^{-3}\)</span>。对梯度进行裁剪（by norm？），限制为 10.0。</p><p>我们将测试 3 个版本的数据。</p><ul><li>0% 的标签，我们忽略数据集中所有的标签；</li><li>10% 的标签，我们对训练和验证集的异常标签进行下采样，以使其包含 10% 的标记异常。这里需要注意的事，缺失点不会进行下采样。我们会不断丢弃异常片段，其概率与每个片段的长度成正比，直到达到所需的下采样率为止。我们使用这种方法，而不是随机丢弃单个异常点，是因为 KPI 是时间序列，因此每个异常点都可能泄漏其相邻点的有关信息，从而导致性能被高估。这样的下采样完成 10 次。</li><li>100% 的标签。</li></ul><h3 id="实验结果">实验结果</h3><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-28_14-13-12.png" style="zoom:40%;" /></p><h2 id="reference">Reference</h2><p><b id="f1">[1]:</b> Jonathan AC Sterne, Ian R White, John B Carlin, Michael Spratt, Patrick Royston, Michael G Kenward, Angela M Wood, and James R Carpenter. 2009. Multiple imputation for missing data in epidemiological and clinical research: potential and pitfalls. Bmj 338 (2009), b2393. <a href="#a1">↩</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> AIOps </tag>
            
            <tag> 异常检测 </tag>
            
            <tag> 论文阅读 </tag>
            
            <tag> 无监督 </tag>
            
            <tag> VAE </tag>
            
            <tag> 变分自编码器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>『阅读笔记』Saliency Detection - A Spectral Residual Approach</title>
      <link href="/2020/04/24/cvpr07_saliency_detection/"/>
      <url>/2020/04/24/cvpr07_saliency_detection/</url>
      
        <content type="html"><![CDATA[<p>这篇 paper 提出了一种简单的视觉显著性检测方法<sup id="a1"><a href="#f1">1</a></sup>。通过分析输入图像的对数频谱，提取图像在光谱域中的频谱残差，能够快速在空间域中构造对应的显著图。该方法可以用作图像中的物体检测。</p><h2 id="背景介绍">背景介绍</h2><p>传统的模型通过将特定的特征与目标联系起来，实际上将物体检测的问题转化为特定种类的物体检测。由于这些模型是基于训练的，因此可扩展性成为通用性任务的瓶颈。面对视觉模式的不可预测性和众多类别，需要通用的显著性检测系统。换句话说，显著性检测器应该以最少的对象统计信息来实现。</p><p>为了找到给定图像中的「原型对象」，基于 Treisman 的整合理论，Itti 和 Koch 提出了一个显著性模型来模拟人类的视觉搜索过程。Walther 在此基础上扩展了显著性模型，并将其成功地应用在对象识别任务上。但是，作为预处理系统，这些模型在计算上要求很高。</p><h2 id="频谱残差模型">频谱残差模型</h2><h3 id="对数频谱表示">对数频谱表示</h3><p>在自然图像统计的不变因素中，尺度不变是最著名和研究最广泛的属性。此属性又称为 <span class="math inline">\(1 / f\)</span> 定律。它指出自然图像集合的平均傅里叶光谱的振幅 <span class="math inline">\(\mathcal{A}(f)\)</span> 服从以下分布： <span class="math display">\[E\{\mathcal{A}(f)\} \propto 1 / f\]</span> 在 log-log 尺度上，对各个方向进行平均后，自然图像整体的振幅频谱大致位于一条直线上。</p><p>尽管 log-log 频谱在理论上已经成熟并且被广泛使用，但它在单个图像的分析中并不受欢迎，因为：</p><ol type="1"><li>在单个图像中不太可能发现尺度不变性；</li><li>采样点分布不均，低频部分在 log-log 平面上稀疏分布，而高频部分汇聚在一起，因而受到噪声的影响。</li></ol><p>本文采用图像的对数频谱 <span class="math inline">\(\mathcal{L}(f)\)</span> 取代 log-log 表示。可以通过 <span class="math inline">\(\mathcal{L}(f)=\log (\mathcal{A}(f))\)</span> 来获得对数频谱。图 1 展示了 log-log 和对数频谱表示的差别。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-24_18-25-20.png" style="zoom:35%;" /></p><p>对数频谱表示已用于与统计分析相关的场景中，而我们将对数频谱应用在显著性检测的任务中。如图 2 所示，尽管每个图像都包含统计奇点，但不同图像的对数频谱具有相似的趋势。图 3 分别显示了 1、10 和 100 张图像上的平均光谱曲线，该结果表明平均对数频谱中的局部线性。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-24_18-31-51.png" style="zoom:40%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-24_18-32-38.png" style="zoom:50%;" /></p><h3 id="从频谱残差到显著图">从频谱残差到显著图</h3><p>相似之处意味着冗余。对于旨在最小化冗余视觉信息的系统，它必须意识到输入刺激的统计相似性。因此，在不同的对数频谱中，尽管可以观察到很大的形状相似性，值得我们注意的确实跳出平滑曲线的信息。我们认为频谱中的统计奇异性可能是图像中异常区域中「原型对象」所导致的。</p><p>给定输入图像，对数频谱 <span class="math inline">\(\mathcal{L}(f)\)</span> 是根据图像降采样到高度（或宽度）为 64px 后计算得出的。输入大小的选择与视觉比例有关。</p><p>如果预先获得了 <span class="math inline">\(\mathcal{L}(f)\)</span> 中包含的信息，则需要处理的信息为： <span class="math display">\[H(\mathcal{R}(f))=H(\mathcal{L}(f) | \mathcal{A}(f))\]</span> 其中，<span class="math inline">\(\mathcal{A(f)}\)</span> 表示对数频谱的一般形状，作为先验信息给出。<span class="math inline">\(\mathcal{R(f)}\)</span> 表示输入图像的统计奇点，在本文中，我们将 <span class="math inline">\(\mathcal{R(f)}\)</span> 定义为光谱残差。</p><p>如图 3 所示，平均曲线表示局部线性。因此，采用局部平均滤波器 <span class="math inline">\(h_n(f)\)</span> 来近似 <span class="math inline">\(\mathcal{A(f)}\)</span> 的形状是合理的。在我们的实验中，<span class="math inline">\(n\)</span> 等于 3。改变 <span class="math inline">\(h_n(f)\)</span> 的大小只会稍微改变结果。平均光谱 <span class="math inline">\(\mathcal{A(f)}\)</span> 可以通过对输入图像进行卷积来近似： <span class="math display">\[\mathcal{A}(f)=h_{n}(f) * \mathcal{L}(f)\]</span> 其中，<span class="math inline">\(h_n(f)\)</span> 是一个 <span class="math inline">\(n \times n\)</span> 矩阵，定义如下： <span class="math display">\[h_{n}(f)=\frac{1}{n^{2}}\left(\begin{array}{cccc}1 &amp; 1 &amp; \dots &amp; 1 \\1 &amp; 1 &amp; \dots &amp; 1 \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\1 &amp; 1 &amp; \dots &amp; 1\end{array}\right)\]</span> 在我们的模型中，光谱残差包含图像的创新。它的作用类似于场景的压缩表示。使用傅里叶逆变换，我们可以在空间域中构造成为显著图的输出图像。显著图主要包含场景中的重要部份。残差光谱的内容也可以解释为图像的意外部分。因此，对显著图中的每个点的值进行平方来表示估计误差。为了获得更好地视觉效果，我们使用高斯滤波器 <span class="math inline">\(g(x),(\sigma=8)\)</span> 来平滑显著图。</p><p>综上所述，给出图像 <span class="math inline">\(\mathcal{I(x)}\)</span>，我们有下式： <span class="math display">\[\begin{aligned}&amp;\begin{array}{l}\mathcal{A}(f)=\Re(\mathfrak{F}[\mathcal{I}(x)]) \\\mathcal{P}(f)=\Im(\mathfrak{F}[\mathcal{I}(x)]) \\\mathcal{L}(f)=\log (\mathcal{A}(f)) \\\mathcal{R}(f)=\mathcal{L}(f)-h_{n}(f) * \mathcal{L}(f)\end{array}\\&amp;\mathcal{S}(x)=g(x) * \mathfrak{F}^{-1}[\exp (\mathcal{R}(f)+\mathcal{P}(f))]^{2}\end{aligned}\]</span> 其中，<span class="math inline">\(\mathfrak{F}\)</span> 和 <span class="math inline">\(\mathfrak{F^{-1}}\)</span> 表示傅里叶变换和傅里叶逆变换；<span class="math inline">\(\mathcal{P(f)}\)</span> 表示图像的相位频谱。</p><h3 id="在显著图中检测原型对象">在显著图中检测原型对象</h3><p>显著图是原型对象的显示表示。在本节中，我们使用简单的阈值分割来检测显著图中的原型对象。给定图像的 <span class="math inline">\(\mathcal{S(x)}\)</span>，对象图 <span class="math inline">\(\mathcal{O(x)}\)</span> 如下式表示： <span class="math display">\[\mathcal{O}(x)=\left\{\begin{array}{ll}1 &amp; \text { if } \mathcal{S}(x)&gt;\text { threshold } \\0 &amp; \text { otherwise }\end{array}\right.\]</span> 根据经验，我们将阈值设置为 <span class="math inline">\(E(\mathcal{S(x)}) \times 3\)</span>，其中 <span class="math inline">\(E(\mathcal{S(x)})\)</span> 是显著图的平均强度。阈值的选择是误报和漏报之间的权衡。</p><p>在生成对象图 <span class="math inline">\(\mathcal{O(x)}\)</span> 时，可以轻松地从输入图像中相应的位置提取原型对象，存在多个目标时按顺序进行提取。根据模拟实验发现，当输入图像宽度（或高度）为 64px 时，可以很好估计视觉条件的规模。</p><h2 id="实验评估">实验评估</h2><p>与 Itti 等人所提出的方法相比，本文提出的光谱残差方法的效果更好，也能很好识别图像中的物体。具体的实验结果可以参照原文，在这里就不多加赘述了。</p><h2 id="reference">Reference</h2><p><b id="f1">[1]:</b> Hou, Xiaodi, and Liqing Zhang. &quot;Saliency detection: A spectral residual approach.&quot; <em>2007 IEEE Conference on computer vision and pattern recognition</em>. Ieee, 2007. <a href="#a1">↩</a></p><hr />]]></content>
      
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
            <tag> 物体检测 </tag>
            
            <tag> 傅里叶变换 </tag>
            
            <tag> 图像检测 </tag>
            
            <tag> 显著图 </tag>
            
            <tag> 频谱残差（SR） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>『阅读笔记』A Deep Neural Network for Unsupervised Anomaly Detection</title>
      <link href="/2020/04/22/aaai19_unsupervised_reading_report/"/>
      <url>/2020/04/22/aaai19_unsupervised_reading_report/</url>
      
        <content type="html"><![CDATA[<p>这篇 paper 提出了一种多尺度卷积递归编码-解码器（Multi-Scale Convolutional Recurrent Encoder-Decoder, MSCRED），可以对多元时间序列数据进行异常检测和诊断。具体来说，MSCRED 首先构建多尺度（分辨率）的签名矩阵，来刻画不同时间步长中系统状态的多个级别。之后，在给定签名矩阵的情况下，使用卷积编码器对传感器（时间序列）之间的相关性进行编码，并开发了基于注意力的卷积 LSTM（ConvLSTM）网络来捕获其中的时间模式。最后，基于对传感器间的相关性和时间信息进行编码从而得到的特征图，使用卷积解码器重建输入签名矩阵，并进一步用残差签名矩阵进行异常检测和诊断。</p><h2 id="背景介绍">背景介绍</h2><p>主要目的是对多元的时间序列数据进行异常检测。图 1 展示了多元时间序列中的两个异常点 <span class="math inline">\(A_1\)</span> 和 <span class="math inline">\(A_2\)</span>，其根本原因分别是黄色和黑色时间序列的异常，<span class="math inline">\(A_2\)</span> 的持续时间大于 <span class="math inline">\(A_1\)</span>，也可以说明 <span class="math inline">\(A_2\)</span> 的严重性级别大于 <span class="math inline">\(A_1\)</span>。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-25_16-02-30.png" style="zoom:40%;" /></p><p>直觉上来说，若 MSCRED 以前从未观察到类似的系统状态，则可能无法很好地重建签名矩阵。图 1 分别展示了正常序列和异常序列的签名矩阵 <span class="math inline">\(M_\text{normal}\)</span> 和 <span class="math inline">\(M_\text{abnormal}\)</span>。理想情况下，MSCRED 无法很好地重建 <span class="math inline">\(M_\text{abnormal}\)</span>，因为用于训练的矩阵（<span class="math inline">\(M_\text{normal}\)</span>）不同于 <span class="math inline">\(M_\text{abnormal}\)</span>。</p><h3 id="相关工作">相关工作</h3><p>过去的无监督异常检测方法由于以下原因无法有效的进行异常检测：</p><ol type="1"><li>多元时间序列中存在时间依赖性，因此以下方法因为无法捕获不同时间步长的时间相关性而可能效果不佳：距离/聚类方法（kNN）、分类方法（One-Class SVM）、密度估计方法（深度自动编码高斯混合模型 Deep Autoencoding Gaussian Mixture Model, DAGMM）。</li><li>多元时间序列数据通常在实际场景下包含噪声。当噪声变得相对严重时，可能会影响时间预测模型的泛化能力并增加误报的可能性，如自回归移动平均值（Autoregressive Moving Average, ARMA）和 LSTM encoder-decoder。</li><li>在实际应用中，根据不同事件的严重性提供不同级别的异常评分很有意义。现有的根因分析方法，如 Ranking Causal Anomalies（RCA），因对噪声敏感而无法处理此类问题。</li></ol><h2 id="mscred-模型架构">MSCRED 模型架构</h2><p>在本节中，我们首先介绍要研究的问题，然后详细阐述 MSCRED 模型的结构。具体来说，我们首先展示如何生成多尺度（分辨率）的系统签名矩阵。然后，我们通过卷积编码器将空间信息编码进签名矩阵中，并通过基于注意力的 ConvLSTM 对时间信息进行建模。最后，我们基于卷积解码器重建签名矩阵，并使用平方损失函数进行端到端学习。</p><h3 id="问题描述">问题描述</h3><p>给出 <span class="math inline">\(n\)</span> 个长度为 <span class="math inline">\(T\)</span> 的历史时间序列 <span class="math inline">\(X = (x_1, x_2, \cdots, x_n)^T \in \mathbb{R}^{n \times T}\)</span>，并假设数据中没有异常点存在，我们的主要目标有两个：</p><ul><li>异常检测：在时间 <span class="math inline">\(T\)</span> 后的某个时间步长内检测出异常事件</li><li>异常诊断：给出检测结果后，确定每个异常点最有可能是哪个异常时间序列造成的，并定性的解释异常严重性（持续时间尺度）。</li></ul><h3 id="签名矩阵">签名矩阵</h3><p>先前的研究表明，不同时间序列对之间的相关性对于表征系统状态至关重要。为了表示从 <span class="math inline">\(t-w\)</span> 到 <span class="math inline">\(t\)</span> 的多元时间序列片段中不同时间序列对之间的相互关系，我们基于该片段内两个时间序列的成对内积来构造 <span class="math inline">\(n \times n\)</span> 的签名矩阵 <span class="math inline">\(M^t\)</span>。具体来说，给出两个时间序列 <span class="math inline">\(\mathbf{x}_{i}^{w}=\left(x_{i}^{t-w}, x_{i}^{t-w-1}, \cdots, x_{i}^{t}\right)\)</span> 和 <span class="math inline">\(\mathbf{x}_{j}^{w}=\left(x_{j}^{t-w}, x_{j}^{t-w-1}, \cdots, x_{j}^{t}\right)\)</span>，它们是多元时间序列 <span class="math inline">\(X^w\)</span> 中的片段。那么它们之间的相关系数 <span class="math inline">\(m_{i j}^{t} \in M^{t}\)</span> 按下式进行计算： <span class="math display">\[m_{i j}^{t}=\frac{\sum_{\delta=0}^{w} x_{i}^{t-\delta} x_{j}^{t-\delta}}{\kappa}\]</span> 其中，<span class="math inline">\(\kappa\)</span> 是尺度参数（<span class="math inline">\(\kappa=w\)</span>）。签名矩阵 <span class="math inline">\(M^t\)</span> 不仅可以捕获两个时间序列之间的形状相似性和尺度相关性，而且由于某些时间序列的湍流对签名矩阵的影响很小，因此它对于输入噪声也很鲁棒。在这篇文章中，我们将两个段之间的间隔设置为 10。此外，为了刻画不同规模的系统状态，我们在每时间步长中构造具有不同长度（<span class="math inline">\(w=10,30,60\)</span>）的签名矩阵 <span class="math inline">\(s\)</span> 个（<span class="math inline">\(s=3\)</span>）。</p><h3 id="卷积编码器">卷积编码器</h3><p>我们采用了全卷积编码器<sup id="a1"><a href="#f1">1</a></sup>对系统的签名矩阵进行空间编码。具体来说，我们将签名矩阵 <span class="math inline">\(M^t\)</span> 拼接到不同尺度作为张量 <span class="math inline">\(\mathcal{X}^{t, 0} \in \mathbb{R}^{n \times n \times s}\)</span>，然后将它们作为卷积层的输入。假设张量 <span class="math inline">\(\mathcal{X}^{t, l-1} \in \mathbb{R}^{n_{l-1} \times n_{l-1} \times d_{l-1}}\)</span> 表示第 <span class="math inline">\((l-1)\)</span> 层的特征图，则第 <span class="math inline">\(l\)</span> 层的输入由下式给出： <span class="math display">\[\mathcal{X}^{t, l}=f\left(W^{l} * \mathcal{X}^{t, l-1}+b^{l}\right)\]</span> 其中，<span class="math inline">\(*\)</span> 表示卷积操作，<span class="math inline">\(f(\cdot)\)</span> 表示激活函数。<span class="math inline">\(W^{l} \in \mathbb{R}^{k_{l} \times k_{l} \times d_{l-1} \times d_{l}}\)</span> 表示 <span class="math inline">\(d_l\)</span> 层卷积核大小为 <span class="math inline">\(k_{l} \times k_{l} \times d_{l-1}\)</span>；<span class="math inline">\(b^{l} \in \mathbb{R}^{d_{l}}\)</span> 是偏置项，<span class="math inline">\(\mathcal{X}^{t, l} \in \mathbb{R}^{n_{l} \times n_{l} \times d_{l}}\)</span> 表示第 <span class="math inline">\(l\)</span> 层输出的特征图。在这篇文章中，我们使用 Scaled Exponential Linear Unit (SELU) 作为激活函数，并使用 4 个卷积层，具体来说，Conv1 到 Conv4 分别使用 32 个 <span class="math inline">\(3 \times 3 \times 3\)</span> 的卷积核、64 个 <span class="math inline">\(3 \times 3 \times 32\)</span> 的卷积核、128 个 <span class="math inline">\(2 \times 2 \times 64\)</span> 的卷积核和 256 个 <span class="math inline">\(2 \times 2 \times 128\)</span> 的卷积核，其中步长分别是 <span class="math inline">\(1 \times 1,2 \times 2,2 \times 2\)</span> 和 <span class="math inline">\(2 \times 2\)</span>。这里要注明的是，形成签名矩阵所依据的时间序列的确切顺序并不重要，因为对于任何给定的排列，卷积编码器都可以捕获所有的局部模式。详细编码过程如图 2(a) 所示。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-25_20-55-59.png" /></p><h3 id="基于-attention-的-convlstm">基于 attention 的 ConvLSTM</h3><p>卷积编码器生成的空间特征图在时间上是取决于之前的时间步长。尽管 ConvLSTM 可以捕获视频序列中的时间信息，但随着序列长度的增加，其性能可能会下降。为了解决这个问题，我们开发了基于 attention 的 ConvLSTM，它可以在不同的时间步长下，自适应地选择相关的隐藏状态（特征图）。具体而言，给定第 <span class="math inline">\(l\)</span> 个卷积层的特征图 <span class="math inline">\(\mathcal{X}^{t, l}\)</span> 和先前的隐藏状态 <span class="math inline">\(\mathcal{H}^{t-1, l} \in \mathbb{R}^{n_{l} \times n_{l} \times d_{l}}\)</span>，当前隐藏层状态 <span class="math inline">\(\mathcal{H}^{t, l}\)</span> 按下式进行更新 <span class="math inline">\(\mathcal{H}^{t, l}=\operatorname{ConvLSTM}\left(\mathcal{X}^{t, l}, \mathcal{H}^{t-1, l}\right)\)</span>，其中 ConvLSTM 的单元如下式计算： <span class="math display">\[\begin{aligned}\mathbf{z}^{t, l}=&amp; \sigma\left(\tilde{W}_{\mathcal{X} \mathcal{Z}}^{l} * \mathcal{X}^{t, l}+\tilde{W}_{\mathcal{H} \mathcal{Z}}^{l} * \mathcal{H}^{t-1, l}+\tilde{W}_{\mathcal{C} \mathcal{Z}}^{k} \circ \mathcal{C}^{t-1, l}+\tilde{b}_{\mathcal{Z}}^{l}\right) \\\mathbf{r}^{t, l}=&amp; \sigma\left(\tilde{W}_{\mathcal{X} \mathcal{R}}^{l} * \mathcal{X}^{t, l}+\tilde{W}_{\mathcal{H} \mathcal{R}}^{l} * \mathcal{H}^{t-1, l}+\tilde{W}_{\mathcal{C} \mathcal{R}}^{l} \circ \mathcal{C}^{t-1, l}+\tilde{b}_{\mathcal{R}}^{l}\right) \\\mathcal{C}^{t, l}=&amp; \mathbf{z}^{t, l} \circ \tanh \left(\tilde{W}_{\mathcal{X} \mathcal{C}}^{l} * \mathcal{X}^{t, l}+\tilde{W}_{\mathcal{H} \mathcal{C}}^{l} * \mathcal{H}^{t-1, l}+\tilde{b}_{\mathcal{C}}^{l}\right)+\\&amp; \mathbf{r}^{t, l} \circ \mathcal{C}^{t-1, l} \\\mathbf{o}^{t, l}=&amp; \sigma\left(\tilde{W}_{\mathcal{X} \mathcal{O}}^{l} * \mathcal{X}^{t, l}+\tilde{W}_{\mathcal{H} \mathcal{O}} * \mathcal{H}^{t-1, l}+\tilde{W}_{\mathcal{C}\mathcal{O}}  \circ \mathcal{C}^{t, l}+\tilde{b}_{\mathcal{O}}^{l}\right) \\\mathcal{H}^{t, l}=&amp; \mathbf{o}^{t, l} \circ \tanh \left(\mathcal{C}^{t, l}\right)\end{aligned}\]</span> 其中，<span class="math inline">\(*\)</span> 表示卷积计算，<span class="math inline">\(\circ\)</span> 表示 Hadamard 乘积，<span class="math inline">\(\sigma\)</span> 表示 Sigmoid 函数。</p><p><span class="math inline">\(\tilde{W}_{\mathcal{X} \mathcal{Z}}^{l}, \tilde{W}_{\mathcal{H Z}}^{l}, \tilde{W}_{\mathcal{C} Z}^{l}, \tilde{W}_{\mathcal{X} \mathcal{R}}^{l}, \tilde{W}_{\mathcal{H} \mathcal{R}}^{l}, \tilde{W}_{\mathcal{C} \mathcal{R}}^{l}, \tilde{W}_{\mathcal{XC}}^{l}, \tilde{W}_{\mathcal{H} \mathcal{C}}^{l}, \tilde{W}_{\mathcal{X} \mathcal{O}}^{l}, \tilde{W}_{\mathcal{H} \mathcal{O}}^{l}, \tilde{W}_{\mathcal{C} \mathcal{O}}^{l} \in \mathbb{R}^{\tilde{k}_{l} \times \tilde{k}_{l} \times \tilde{d}_{l} \times \tilde{d}_{l}}\)</span> 分别是 <span class="math inline">\(\tilde{d}_{l}\)</span> 的卷积核，其大小为 <span class="math inline">\(\tilde{k}_{l} \times \tilde{k}_{l} \times \tilde{d}_{l}\)</span>；<span class="math inline">\(\tilde{b}_{\mathcal{Z}}^{l}, \tilde{b}_{\mathcal{R}}^{l}, \tilde{b}_{\mathcal{G}}^{l}, \tilde{b}_{\mathcal{Q}}^{l} \in \mathbb{R}^{\tilde{d}_{l}}\)</span> 是 ConvLSTM 第 <span class="math inline">\(l\)</span> 层的偏置参数。在这篇文章中，我们在每一层上都保持与卷积编码器相同的卷积核大小。这里需要注明的是，所有的输入 <span class="math inline">\(\mathcal{X}^{t, l}\)</span>、单元输出 <span class="math inline">\(\mathcal{C}^{t, l}\)</span>、隐藏状态 <span class="math inline">\(\mathcal{H}^{t-1, l}\)</span> 和门 <span class="math inline">\(\mathbf{z}^{t, l}, \mathbf{r}^{t, l}, \mathbf{o}^{t, l}\)</span> 都是 3D 张量，与 LSTM 不同。我们调整了步长参数 <span class="math inline">\(h\)</span>（即之前的片段数），并根据实验将其设置为 5。</p><p>此外，考虑到并非所有之前的步长都与当前状态 <span class="math inline">\(\mathcal{H}^{t, l}\)</span> 均等相关，我们采用时间注意机制来自适应地选择与当前步长相关的步长，并合计这些信息特征图从而形成经过精调的特征图输出 <span class="math inline">\(\hat{\mathcal{H}}^{t-1, l}\)</span>： <span class="math display">\[\hat{\mathcal{H}}^{t, l}=\sum_{i \in(t-h, t)} \alpha^{i} \mathcal{H}^{i, l}, \alpha^{i}=\frac{\exp \left\{\frac{\operatorname{Vec}\left(\mathcal{H}^{t, l}\right)^{\mathrm{T}} \operatorname{Vec}\left(\mathcal{H}^{i, l}\right)}{\chi}\right\}}{\sum_{i \in(t-h, t)} \exp \left\{\frac{\operatorname{Vec}\left(\mathcal{H}^{t, l}\right)^{\mathrm{T}} \operatorname{Vec}\left(\mathcal{H}^{i, l}\right)}{\chi}\right\}}\]</span> 其中，<span class="math inline">\(Vec(\cdot)\)</span> 表示向量，<span class="math inline">\(\chi\)</span> 是重缩放因子（<span class="math inline">\(\chi\)</span> = 5.0）。也就是说，我们将最后的隐藏状态 <span class="math inline">\(\mathcal{H}^{t, l}\)</span> 作为组级别的上下文向量，并通过 Softmax 函数测量先前步长的重要性权重 <span class="math inline">\(\alpha_i\)</span>。与引入转换和上下文参数的一般注意力机制不同，上述公式仅基于学习到的隐藏特征图，并实现了与前者相似的功能。本质上，基于 attention 的 ConvLSTM 在每个卷积层使用时间信息共同对签名矩阵的空间模式进行建模。图 2(b) 展示了时间建模的过程。</p><h3 id="卷积解码器">卷积解码器</h3><p>为了对上一步中获得的特征图进行解码并获得重构的签名矩阵，我们设计了一个卷积解码器，其公式为： <span class="math display">\[\hat{\mathcal{X}}^{t, l-1}=\left\{\begin{aligned}&amp; f\left(\hat{W}^{t, l} \otimes \hat{\mathcal{H}}^{t, l}+\hat{b}^{t, l}\right), &amp; l =4 \\&amp; f\left(\hat{W}^{t, l} \otimes\left[\hat{\mathcal{H}}^{t, l}\right.\right.\left.\left.\oplus \hat{\mathcal{X}}^{t, l}\right]+\hat{b}^{t, l}\right), &amp; l =3,2,1\end{aligned}\right.\]</span> 其中，<span class="math inline">\(\otimes\)</span> 表示逆卷积运算，<span class="math inline">\(\oplus\)</span> 表示级联运算，<span class="math inline">\(f(\cdot)\)</span> 是激活单元（与编码器相同）。<span class="math inline">\(\hat{W}^{l} \in \mathbb{R}^{\hat{k}_{l} \times \hat{k}_{l} \times \hat{d}_{l} \times \hat{d}_{l-1}}\)</span> 和 <span class="math inline">\(\hat{b}^{l} \in \mathbb{R}^{\hat{d}_{l}}\)</span> 分别是第 <span class="math inline">\(l\)</span> 个反卷积层的卷积核和偏置参数。具体来说，我们遵循相反的顺序，将第 <span class="math inline">\(l\)</span> 层的 ConvLSTM <span class="math inline">\(\mathcal{H}^{t, l}\)</span> 反馈到反卷积神经网络中。输出的特征图 <span class="math inline">\(\hat{\mathcal{X}}^{t, l-1} \in \mathbb{R}^{\hat{n}_{l-1} \times \hat{n}_{l-1} \times \hat{d}_{l-1}}\)</span> 与先前的 ConvLSTM 层的输出串联在一起，从而使解码器堆叠在一起。级联表示将结果进一步馈入下一个反卷积层。最终输出 <span class="math inline">\(\hat{\mathcal{X}}^{t, 0} \in \mathbb{R}^{n \times n \times s}\)</span>（与输入的矩阵相同大小）表示重构的签名矩阵。我们使用 4 个反卷积层，DeConv4 到 DeConv1，分别具有 128 个大小为 <span class="math inline">\(2 \times 2 \times 256\)</span> 的卷积核、 64 个大小为 <span class="math inline">\(2 \times 2 \times 128\)</span> 的卷积核、32 个大小为 <span class="math inline">\(3 \times 3 \times 64\)</span> 的卷积核以及 3 个大小为 <span class="math inline">\(3 \times 3 \times 64\)</span> 的卷积核，步长分别为 <span class="math inline">\(2 \times 2, 2 \times 2, 2 \times 2\)</span> 和 <span class="math inline">\(1 \times 1\)</span>。解码器能够在不同的反卷积层上合并特征图，这将有效提高异常检测的性能。图 2(c) 展示了解码的过程。</p><h3 id="损失函数">损失函数</h3><p>MSCRED 的损失函数定义为签名矩阵的重构误差，即： <span class="math display">\[\mathcal{L}_{M S C R E D}=\sum_{t} \sum_{c=1}^{s}\left\|\mathcal{X}_{:, ;, c}^{t, 0}-\hat{\mathcal{X}}_{:,,, c}^{t, 0}\right\|_{F}^{2}\]</span> 其中，<span class="math inline">\(\mathcal{X}_{:, ;, c}^{t, 0} \in \mathbb{R}^{n \times n}\)</span>。我们将小批量随机梯度下降法与 Adam 优化器一起使用来最小化损失函数。经过足够的训练之后，讲学习到的神经网络参数用于验证集和测试集的签名函数的重构。最后，我们基于残差签名矩阵来进行异常检测和诊断。</p><h2 id="实验评估">实验评估</h2><p>使用两种数据集进行评估，分别是模拟数据集和电厂的数据集。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-26_11-05-30.png" style="zoom:40%;" /></p><ul><li><span class="math inline">\(\text{CNN}_\text{ConvLSTM}^\text{ED(4)}\)</span> 表示前三层的 ConvLSTM 被移除，使用 attention 模块的 MSCRED 。</li><li><span class="math inline">\(\text{CNN}_\text{ConvLSTM}^\text{ED(3,4)}\)</span> 表示前两层的 ConvLSTM 被移除，使用 attention 模块的 MSCRED 。</li><li><span class="math inline">\(\text{CNN}_\text{ConvLSTM}^\text{ED}\)</span> 表示不使用 attention 模块的 MSCRED。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-26_10-18-50.png" style="zoom:40%;" /></p><p>为了检测异常，我们遵循领域内专家的建议，设置一个阈值 <span class="math inline">\(\tau=\beta \cdot max\{s(t)_{\text{valid}}\}\)</span>，其中 <span class="math inline">\(s(t)_{\text{valid}}\)</span> 是验证集上的异常得分，而 <span class="math inline">\(\beta \in [1, 2]\)</span> 设置为使得验证集的 F1 分数最大。根据该阈值，计算得出测试集上的准确率和召回率。将两个数据集上的实验重复 5 次，并使用平均结果进行比较。需要注意的是，MSCRED 的输出包含三个通道的残差签名矩阵 <span class="math inline">\(w.r.t\)</span>，都是不同的片段长度。我们使用最小的（<span class="math inline">\(w=10\)</span>）进行异常检测和根因评估。我们还将提供三个通道结果的性能比较，以解释异常严重性。</p><p>MSCRED 的签名矩阵包括 <span class="math inline">\(s=3\)</span> 个通道，从而在不同的尺度上捕获系统状态。为了解释异常严重性，我们首先基于三个通道的残差签名矩阵来计算异常评分，三个通道的片段大小分别为 <span class="math inline">\(w=10,30,60\)</span>，以 MSCRED(S)、MSCRED(M) 和 MSCRED(L) 进行表示。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-26_10-58-25.png" style="zoom:40%;" /></p><p>从图 6 中可以看到，MSCRED(S) 能够检测到所有类型的异常，MSCRED(M) 可以检测中长期的异常，而 MSCRED(L) 只能检测到长时间的异常。因此，我们可以通过共同考虑三个异常得分来解释异常严重性。如果在三个通道中都检测到异常，表示异常持续时间可能较长。</p><p>图 7 提供了一个案例研究，展示了电厂数据中的异常诊断。在这种情况下，MSCRED(S) 会检测到所有的 5 种异常，包括 3 个短期、1 个中期和 1 个长期异常；MSCRED(M) 未能检测出两个短期异常，而 MSCRED(L) 仅能检测到 1 个长期异常。此外，注入的异常事件的四个残差签名矩阵显示了根本原因的识别结果。在这种情况下，我们可以准确查明一半以上的异常根本原因（用红色矩阵突显出的行/列）。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-26_10-56-09.png" style="zoom:40%;" /></p><h2 id="reference">Reference</h2><p>Long, J.; Shelhamer, E.; and Darrell, T. 2015. Fully convolutional networks for semantic segmentation. In CVPR, 3431–3440.</p><p>Bahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural machine translation by jointly learning to align and translate. In ICLR.</p>]]></content>
      
      
      
        <tags>
            
            <tag> AIOps </tag>
            
            <tag> 根因分析 </tag>
            
            <tag> 异常检测 </tag>
            
            <tag> 论文阅读 </tag>
            
            <tag> ConvLSTM </tag>
            
            <tag> attention </tag>
            
            <tag> Encoder-Decoder </tag>
            
            <tag> 多元时间序列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>『阅读笔记』Time-Series Anomaly Detection Service at Microsoft</title>
      <link href="/2020/04/20/kdd19_microsoft_reading_report/"/>
      <url>/2020/04/20/kdd19_microsoft_reading_report/</url>
      
        <content type="html"><![CDATA[<p>这篇 paper 由微软团队在 2019 年发表<sup id="a1"><a href="#f1">1</a></sup>。 他们提出了基于频谱残差（Spectual Residual, SR）和卷积神经网络（CNN）的异常检测算法，提供了一种非监督异常检测的新思路。</p><h2 id="挑战">挑战</h2><h3 id="缺乏标签">缺乏标签</h3><p>仅靠人力难以手动标记每个时间序列的异常部分。而且，时间序列的分布是不断变化的，即使是以前从未出现过的模式也要求能够识别出来，因此，只使用监督模型在工业场景中并不足够。</p><h3 id="一般化">一般化</h3><p>时间序列有几种典型的模式类别。对于异常检测服务来说，在各种时间序列模式下能够正常工作十分重要。然而，目前还没有方法能够足够的概括所有不同的模式。例如，Holt-Winters 方法在 (b) 和 (c) 模式下表现不佳，Spot 方法在 (a) 模式下也显示不够好的结果。因此，需要一种通用性更好的解决方案。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-20_16-53-57.png" style="zoom:50%;" /></p><center style="font-size:14px;color:#808080;text-decoration:underline">图1. 几种时间序列模式</center><h3 id="效率">效率</h3><p>在业务应用程序中，监控系统必须几乎实时处理数百万甚至数十亿个时间序列，特别是分钟级别的时间序列，异常检测过程需要在有限的时间内完成。因此，即使有一些具有较大时间复杂度的模型在准确率方面也很出色，但是在线上场景中很少使用。</p><h2 id="算法介绍">算法介绍</h2><p>该论文的主要目的是开发一种没有标签数据的通用高效算法。受视觉计算领域的启发，我们采用频谱残差（SR）<sup id="a2"><a href="#f2">2</a></sup>，这是一种基于快速傅里叶变换（FFT）的简单而强大的方法。SR 方法是无监督的，并且已被证明在视觉显著性检测应用中非常有效。我们认为视觉显著性检测和时间序列异常检测任务在本质上是相似的，因为异常点通常在视觉上是显著的。</p><h3 id="sr">SR</h3><p>频谱残差算法主要包括三个步骤：</p><ol type="1"><li>傅立叶变换，从而获得对数幅度频谱</li><li>计算频谱残差</li><li>傅立叶逆变换，将序列变换回空间域</li></ol><p>给定序列 <span class="math inline">\(x\)</span>，我们有如下计算式： <span class="math display">\[\begin{array}{l}A(f)=\text {Amplitude}(\widetilde{\mathfrak{F}}(\mathbf{x})) \\P(f)=\operatorname{Phrase}(\widetilde{\mathfrak{F}}(\mathbf{x})) \\L(f)=\log (A(f)) \\A L(f)=h_{q}(f) \cdot L(f) \\R(f)=L(f)-A L(f) \\S(\mathbf{x})=\left\|\mathfrak{F}^{-1}(\exp (R(f)+i P(f)))\right\|\end{array}\]</span> 其中，<span class="math inline">\(\mathfrak{F}\)</span> 和 <span class="math inline">\(\mathfrak{F}^{-1}\)</span> 分别表示傅立叶变换和傅立叶逆变换；<span class="math inline">\(x\)</span> 是维度为 <span class="math inline">\(n \times 1\)</span> 的输入序列；<span class="math inline">\(A(f)\)</span> 是序列 <span class="math inline">\(x\)</span> 的振幅频谱；<span class="math inline">\(P(f)\)</span> 是序列 <span class="math inline">\(x\)</span> 相对应的相位频谱；<span class="math inline">\(L(f)\)</span> 是 <span class="math inline">\(A(f)\)</span> 的对数表示；<span class="math inline">\(AL(f)\)</span> 是 <span class="math inline">\(L(f)\)</span> 的平均频谱，可以通过与 <span class="math inline">\(h_{q}(f)\)</span> 卷积得到，其中 <span class="math inline">\(h_{q}(f)\)</span> 是一个 <span class="math inline">\(q \times q\)</span> 的矩阵： <span class="math display">\[h_{q}(f)=\frac{1}{q^{2}}\left[\begin{array}{ccccc}1 &amp; 1 &amp; 1 &amp; \dots &amp; 1 \\1 &amp; 1 &amp; 1 &amp; \dots &amp; 1 \\\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\1 &amp; 1 &amp; 1 &amp; \dots &amp; 1\end{array}\right]\]</span> <span class="math inline">\(R(f)\)</span> 是频谱残差，即对数频谱 <span class="math inline">\(L(f)\)</span> 减去平均对数频谱 <span class="math inline">\(AL(f)\)</span>。频谱残差用作序列的压缩表示，从而使得原始序列的创新部分变得更加重要。最后，我们通过逆傅立叶变换将序列转移回空间域，<span class="math inline">\(S(x)\)</span> 称为显著图。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/20200420234023.png" style="zoom:40%;" /></p><center style="font-size:14px;color:#808080;text-decoration:underline">图2. 时间序列 SR 变换示例</center><p>图 2 显示了原始时间序列和经过 SR 处理后的显著图。如图所示，显著图中的创新点（以红色点显示）比原始输入中的创新点重要得多。基于显著图，很容易利用简单的规则正确地标注异常点。我们采用一个简单的阈值 <span class="math inline">\(\tau\)</span> 来进行异常点判断，如下公式所示： <span class="math display">\[O\left(x_{i}\right)=\left\{\begin{array}{ll}1, &amp; \text { if }\left.\frac{S\left(x_{i}\right)-\overline{S\left(x_{i}\right)}}{\overline{S\left(x_{i}\right)}}\right)&gt;\tau \\0, &amp; \text { otherwise }\end{array}\right.\]</span> 其中 <span class="math inline">\(x(i)\)</span> 代表序列中任意点，<span class="math inline">\(S(x_i)\)</span> 是显著图中的对应点，<span class="math inline">\(\overline{S(x_i)}\)</span> 是 <span class="math inline">\(S(x_i)\)</span> 之前 <span class="math inline">\(z\)</span> 个点的局部平均值。</p><p>在实际操作中，FFT 是在序列的滑动窗口内进行的。进一步来说，我们期望该算法能够发现具有低延迟的异常点。也就是说，给定一个序列 <span class="math inline">\(x_1, x_2, \cdots, x_n\)</span>，其中 <span class="math inline">\(x_n\)</span> 是序列中最近的点，因此我们想尽快判断 <span class="math inline">\(x_n\)</span> 是否为一个异常点。然而，当目标点位于滑动窗口的中心时，SR 算法的效果最好。因此，在将序列输入 SR 模型之前，我们会在 <span class="math inline">\(x_n\)</span> 之后添加好几个估计点。估计点 <span class="math inline">\(x_{n+1}\)</span> 由下式计算： <span class="math display">\[\begin{array}{c}\bar{g}=\frac{1}{m} \sum_{i=1}^{m} g\left(x_{n}, x_{n-i}\right) \\x_{n+1}=x_{n-m+1}+\bar{g} \cdot m\end{array}\]</span> 其中，<span class="math inline">\(g(x_i, x_j)\)</span> 表示点 <span class="math inline">\(x_i\)</span> 与 <span class="math inline">\(x_j\)</span> 之间直线的梯度，<span class="math inline">\(\bar{g}\)</span> 表示之前各点的平均梯度，<span class="math inline">\(m\)</span> 是之前点的数量，在实现中我们将 <span class="math inline">\(m\)</span> 设置为 5。我们发现，第一个估计点起着决定性的作用，因此，我们只将 <span class="math inline">\(x_{n+1}\)</span> 复制 <span class="math inline">\(\kappa\)</span> 次，并将这些点添加到序列的尾部。</p><p>综上所述，SR 算法仅包含几个超参数，即滑动窗口大小 <span class="math inline">\(\omega\)</span>，估计点的数量 <span class="math inline">\(\kappa\)</span> 和异常检测阈值 <span class="math inline">\(\tau\)</span>。我们根据经验来设置它们，并在试验中显示其鲁棒性。因此，SR 算法是在线异常检测服务的不错选择。</p><h3 id="sr-cnn">SR-CNN</h3><p>原始 SR 方法利用显著图上的单个阈值来检测异常点，但其规则过于简单，因此需要寻求更复杂的决策规则。我们的理念是通过精心设计的合成数据，训练出异常检测的判别模型。合成数据可以通过向显著图集合中插入不包含在评估数据集中的异常点来实现。因此，注入点会被标注为异常，而其他标记为正常。具体来说，我们在时间序列中随机选择几个点，计算插入值从而替换原始点，并获得其显著图。异常点的值通过以下方式计算： <span class="math display">\[x=(\bar{x}+m e a n)(1+v a r) \cdot r+x\]</span> 其中 <span class="math inline">\(\bar{x}\)</span> 是先前点的本地平均；mean 和 var 是当前滑动窗口内所有点的均值和方差；<span class="math inline">\(r \sim \mathcal{N}(0,1)\)</span>，<span class="math inline">\(r\)</span> 是正态分布的随机采样。</p><p>我们选择 CNN 作为判别模型的架构。CNN 是常用于显著性检测的监督模型。但是，由于场景中通常没有足够的标记数据，因此我们使用基于显著图的 CNN，而不是原始的输入，这使得异常标注的问题更加容易。在实践中，我们使用具有合成异常点的时间序列作为训练集，这样的优点是检测器适应时间序列分布的变化，而无需手动标记的数据。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-21_12-17-56.png" style="zoom:50%;" /></p><center style="font-size:14px;color:#808080;text-decoration:underline">图3. SR-CNN 结构</center><p>SR-CNN 的结构如图 3 所示。该网络由两个一维卷积层（filter 大小等于滑动窗口大小 <span class="math inline">\(\omega\)</span>）和两个全连接层组成。第一个卷积层 channel 大小等于 <span class="math inline">\(\omega\)</span>，第二个卷积层 channel 大小增加一倍。在 Sigmoid 输出前，堆叠了两个全连接层。使用交叉熵作为损失函数，SGD 作为优化器。</p><h2 id="实验评估">实验评估</h2><p>使用三个数据集进行评测，<a href="http://iops.ai/competition_detail/?competition_id=5&amp;flag=1." target="_blank" rel="noopener">AIOPS 竞赛的 KPI 数据集</a>、<a href="https://yahooresearch.tumblr.com/post/114590420346/" target="_blank" rel="noopener">Yahoo 的公共数据集</a>和 Microsoft 的内部数据集。KPI 数据集的时间间隔点为 1 分钟或 5 分钟；Yahoo 数据集中一部分是合成的，另一部分是实际的流量，时间间隔为 1 小时；Microsoft 数据集的时间间隔为 1 天。</p><p>FFT<sup id="a3"><a href="#f3">3</a></sup>（快速傅立叶变换）、Twitter-AD<sup id="a4"><a href="#f4">4</a></sup> 和 Luminol<sup id="a5"><a href="#f5">5</a></sup>（LinkedIn 的异常检测）不需要其他数据即可启动（冷启动）；另一方面，SPOT、DSPOT<sup id="a6"><a href="#f6">6</a></sup> 和 DONUT<sup id="a7"><a href="#f7">7</a></sup> 则需要其他数据来训练模型。因此根据时间顺序将每个时间序列的点分为两半，上半部分由于训练、下半部分由于测试评估。</p><p>我们将 SR 和 SR-CNN 的超参数设置为如下：<span class="math inline">\(h_q(f)\)</span> 中的 <span class="math inline">\(q\)</span> 设为 3，先前点的数量 <span class="math inline">\(z\)</span>（由于计算本地平均值）设为 21、阈值 <span class="math inline">\(\tau\)</span> 设为 3，估计点的数量 <span class="math inline">\(\kappa\)</span> 设为 5。滑动窗口大小 <span class="math inline">\(\omega\)</span> 在 KPI 上设为 1440，在 Yahoo 上设为 64，在 Microsoft 上设为 30。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-21_13-33-07.png" /></p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-21_13-48-27.png" style="zoom:40%;" /></p><p>从表格中可以看到，SR 和 SR-CNN 的效果基本上都是最好的。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-21_13-50-32.png" style="zoom:40%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-21_13-51-38.png" style="zoom:50%;" /></p><p>另外，使用 SR+DNN 在监督训练中，也比单纯的 DNN 表现效果要更好一些。</p><h2 id="reference">Reference</h2><p><b id="f1">[1]:</b> Ren, Hansheng, et al. &quot;Time-Series Anomaly Detection Service at Microsoft.&quot; <em>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 2019. <a href="#a1">↩</a></p><p><b id="f2">[2]:</b> Hou, Xiaodi, and Liqing Zhang. &quot;Saliency detection: A spectral residual approach.&quot; <em>2007 IEEE Conference on computer vision and pattern recognition</em>. Ieee, 2007. <a href="#a2">↩</a></p><p><b id="f3">[3]: </b> Faraz Rasheed, Peter Peng, Reda Alhajj, and Jon Rokne. 2009. Fourier trans- form based spatial outlier mining. In International Conference on Intelligent Data Engineering and Automated Learning. Springer, 317–324. <a href="#a3">↩</a></p><p><b id="f4">[4]:</b> Owen Vallis, Jordan Hochenbaum, and Arun Kejariwal. 2014. A Novel Technique for Long-Term Anomaly Detection in the Cloud. In 6th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 14). USENIX Association, Philadelphia, PA. <a href="#a4">↩</a></p><p><b id="f5">[5]:</b> https://github.com/linkedin/luminol. <a href="#a5">↩</a></p><p><b id="f6">[6]:</b> Alban Siffer, Pierre-Alain Fouque, Alexandre Termier, and Christine Largouet. 2017. Anomaly detection in streams with extreme value theory. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 1067–1075. <a href="#a6">↩</a></p><p><b id="f7">[7]:</b> Haowen Xu, Wenxiao Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li, Ying Liu, Youjian Zhao, Dan Pei, Yang Feng, et al. 2018. Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications. In Proceedings of the 2018 World Wide Web Conference on World Wide Web. Inter- national World Wide Web Conferences Steering Committee, 187–196. <a href="#a7">↩</a></p><hr />]]></content>
      
      
      
        <tags>
            
            <tag> AIOps </tag>
            
            <tag> 异常检测 </tag>
            
            <tag> 论文阅读 </tag>
            
            <tag> 卷积神经网络 </tag>
            
            <tag> CNN </tag>
            
            <tag> 频谱残差（SR） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>『统计学』协方差 and 相关系数</title>
      <link href="/2020/04/11/statistic_intro/"/>
      <url>/2020/04/11/statistic_intro/</url>
      
        <content type="html"><![CDATA[<h2 id="期望值">期望值</h2><p>在概率论和统计学中，一个离散性随机变量的期望值，是试验中每次可能的结果乘以其概率结果的总和。而在实际的应用或案例中，我们通常遇到的情况都是离散的。因此这里只简单介绍一下此种情况的期望值如何计算。</p><h3 id="定义">定义</h3><p>若 <span class="math inline">\(X\)</span> 是离散的随机变量，输出值为 <span class="math inline">\(x_1, x_2, \cdots\)</span>，与输出值相应的概率为 <span class="math inline">\(p_1, p_2, \cdots\)</span>（概率和为1），则 <span class="math inline">\(X\)</span> 的期望值如下： <span class="math display">\[E(X) = \sum_i{p_ix_i}\]</span> 当样本发生概率都相同时（平均分布），期望值又等于均值 <span class="math inline">\(E(X) = \bar{x}\)</span>。</p><h3 id="性质">性质</h3><ul><li><p>期望值是线性函数</p><p><span class="math inline">\(E(aX+bY)=aE(X)+bE(Y)\)</span></p></li><li><p>当随机变量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 的协方差为 0 时（又称它们不相关），下面等式成立。（特别地，当 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 独立时，协方差也为 0）</p><p><span class="math inline">\(E(XY)=E(X) \cdot E(Y)\)</span></p></li></ul><h2 id="协方差">协方差</h2><p>协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。通俗来说，就是两个变量在变化过程中的变化趋势，是同方向变化还是反方向变化？同向或反向程度如何？</p><p>若两个变量是同向变化的，这时协方差就是正的；若两个变量反向变化，这时协方差就是负的。协方差数值越大，表示两个变量的同向程度也越大。</p><h3 id="定义-1">定义</h3><p>若随机变量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 的期望值分别是 <span class="math inline">\(E(X)=\mu, E(Y)=\nu\)</span>，则 <span class="math inline">\(X\)</span> 与 <span class="math inline">\(Y\)</span> 之间的协方差为： <span class="math display">\[Cov(X, Y) = E[(X-\mu)(Y-\nu)] = E(XY) - \mu\nu\]</span> 该公式简单来说就是，两个变量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 的协方差为，「每个时刻的 <span class="math inline">\(X\)</span> 与其期望之差」乘以「对应时刻的 <span class="math inline">\(Y\)</span> 与其期望之差」，最后再对得到的乘积求和并计算其期望值。</p><p>若 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 是统计独立的，那么二者的协方差为 0，这是因为 <span class="math display">\[E(XY) = E(X) \cdot E(Y) = \mu\nu\]</span> 但是反过来并不成立，即若 <span class="math inline">\(X\)</span> 与 <span class="math inline">\(Y\)</span> 的协方差为 0，二者并不一定是统计独立的，但是二者之间一定是不相关的。</p><p>另外，对于 N 个相等概率值的平均分布， <span class="math display">\[Cov(X, Y) = \frac{1}{N} \sum_i^N (x_i-\bar{x})(y_i-\bar{y})\]</span></p><h2 id="方差">方差</h2><p>方差（Variance）是协方差的特殊情况，即两个变量是相同的。在概率论和统计学中，一个随机变量的方差描述的是它的离散程度，也就是该变量与其期望值的距离。方差越大，代表大部分数值与其期望之间的差异较大。</p><h3 id="定义-2">定义</h3><p>若随机变量 <span class="math inline">\(X\)</span> 的期望值 <span class="math inline">\(E(X)=\mu\)</span>，则 <span class="math inline">\(X\)</span> 的方差为： <span class="math display">\[\begin{aligned}Var(X) &amp;= E[(X-\mu)^2] = Cov(X, X)\end{aligned}\]</span> 将上式展开可得： <span class="math display">\[Var(X) = E[X^2-2X \cdot E(X)+[E(X)]^2]=E(X^2)-2E(X) \cdot E(X) + [E(X)]^2 = E(X^2)-[E(X)]^2\]</span> 即 <span class="math inline">\(X\)</span> 的方差为「其平方的期望」减去「其期望的平方」。</p><p>若 <span class="math inline">\(X\)</span> 是离散的随机变量，输出值为 <span class="math inline">\(x_1, x_2, \cdots\)</span>，与输出值相应的概率为 <span class="math inline">\(p_1, p_2, \cdots\)</span>（概率和为1），则 <span class="math inline">\(X\)</span> 的方差如下： <span class="math display">\[\begin{aligned}E(X) &amp;= \sum_i{p_ix_i} = \mu\\Var(X) &amp;= \sum_i p_i \cdot (x_i-\mu)^2 = \sum_i(p_i \cdot {x_i}^2) - \mu^2\end{aligned}\]</span> 而对于 N 个相等概率值的平均分布， <span class="math display">\[Var(X) = \sigma^2 = \frac{1}{N} \sum_i^N(x_i-\bar{x})^2 = \frac{1}{N}\sum_i^N{x_i}^2 - {\bar{x}}^2\]</span></p><h3 id="标准差">标准差</h3><p>我们通常用 <span class="math inline">\(\sigma^2\)</span> 来表示方差，而 <span class="math inline">\(\sigma\)</span> 则表示标准差（Standard Deviation）。</p><p>标准差与方差一样用来表示组内数据间的离散程度，但是相比于方差，标准差用来表示离散程度的数字与样本数据点的数量级和单位一致，更容易理解和后续的分析计算。</p><p>另外，在样本数据大致符合正态分布的情况下，标准差具有方便估算的特性：68.3% 的数据点落在平均值前后 1 个标准差的范围内；95.4% 的数据点落在平均值前后 2 个标准差的范围内；而 99.7% 的数据点将会落在平均值前后 3 个标准差的范围内。因此可以使用 <span class="math inline">\(3\sigma\)</span> 定律排除掉异常的数据点。</p><h2 id="相关系数">相关系数</h2><h3 id="定义-3">定义</h3><p>两个变量之间的皮尔逊相关系数（Pearson Correlation Coeffocient）定义为两个变量之间的协方差和标准差的商，用于度量变量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 之间的线性相关关系，取值范围为 [-1, 1]，其中正数表示正线性相关，负数表示负线性相关，而 0 表示非线性相关。 <span class="math display">\[\rho_{X, Y} = \frac{Cov(X, Y)}{\sigma_X \sigma_Y}\]</span> 因此，相关系数也可以看作成一种协方差：一种剔除了两个变量的量纲影响、标准化后的特殊协方差。它消除了两个变量间变化幅度的影响，更能反应出两个变量每单位变化时的相似程度。</p><p>而对于样本的相关系数，常用 <span class="math inline">\(r\)</span> 来表示： <span class="math display">\[r = \frac{\sum_{i=1}^N (x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^N(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^N(y_i-\bar{y})^2}}\]</span> 下图是几组 (x, y) 的点集，以及各个点集中 x 和 y 之间的相关系数。我们可以发现相关系数反映的是变量之间的线性关系和相关性的方向（第一排），而不是相关性的斜率（中间），也不是各种非线性关系（第三排）。请注意：中间的图中斜率为 0，但相关系数是没有意义的，因为此时变量 Y 是 0。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-12_20-40-17.png" style="zoom:20%;" /></p><center style="font-size:16px;color:#808080;text-decoration:underline">图1. 来自维基百科</center><h2 id="自相关系数">自相关系数</h2><p>正如相关系数可以衡量两个变量之间的线性相关关系一样，自相关系数（Autocorrelation）可以测量时间序列 <strong>滞后值</strong> 之间的线性关系。</p><h3 id="定义-4">定义</h3><p>我们用 <span class="math inline">\(r_k\)</span> 来表示自相关系数，例如 <span class="math inline">\(r_1\)</span> 衡量的是 <span class="math inline">\(y_t\)</span> 和 <span class="math inline">\(y_{t-1}\)</span> 之间的关系， <span class="math inline">\(r_2\)</span> 衡量的是 <span class="math inline">\(y_t\)</span> 和 <span class="math inline">\(y_{t-2}\)</span> 之间的关系。其中 <span class="math inline">\(T\)</span> 是时间序列的长度 <span class="math display">\[r_k = \frac{\sum_{t=k+1}^T (y_t-\bar{y})(y_{t-k}-\bar{y})}{\sqrt{\sum_{t=1}^T(y_t-\bar{y})^2}}\]</span></p><p>自相关系数值越高，代表这两个时间点具有更高的线性相关性。</p><h3 id="自相关图">自相关图</h3><p>通过将自相关系数 <span class="math inline">\(r_k\)</span> 绘制成图，即可得到自相关函数（Autocorrelation），也称作相关图（ACF）。</p><p><img src="https://otexts.com/fppcn/fpp_files/figure-html/aelec-1.png" style="zoom:80%;"   ></p><p>上图表示 1980-1995 年间澳大利亚月度用电量，可以看出该图具有向上的趋势，以及明显的季节性。下图则是其对应的 ACF 图。</p><p><img src="https://otexts.com/fppcn/fpp_files/figure-html/acfelec-1.png" style="zoom:80%;" ></p><p>当数据具有趋势性时，短期滞后的自相关值较大，因为观测点附近的值波动不会很大（也就是说，离观测点近的数据点会和观测点的数值相近）。这时候的时间序列的 ACF 一般是正值，随着滞后阶数的增加而缓慢下降。</p><p>当数据具有季节性时，自相关值在滞后阶数与季节周期相同（或其倍数）时较大。可以看到图中，每年会有一个高峰和低估，因此在滞后阶数为 12 的倍数时，其自相关值会呈现「圆齿状」。</p><h2 id="白噪声">白噪声</h2><p>白噪声是一个对所有时间，其自相关系数为零的随机过程。</p><p><img src="https://otexts.com/fppcn/fpp_files/figure-html/wnoise-1.png" style="zoom: 80%;" ></p><p><img src="https://otexts.com/fppcn/fpp_files/figure-html/wnoiseacf-1.png" style="zoom:80%;" ></p><p>对于白噪声而言，我们期望它的自相关值接近零。但是由于随机扰动的存在，自相关值并不会精确地等于零。对于一个长度为 <span class="math inline">\(T\)</span> 的白噪声序列而言，我们期望在 0.95 的置信度下，它的自相关值处于 <span class="math inline">\(\frac{2}{\sqrt{T}}\)</span> 之间。我们可以很容易的画出 ACF 的边界值（图中蓝色虚线）。如果一个序列中有较多自相关值处于边界之外，那么该序列很可能不是白噪声序列。</p><p>在上例中，序列长度 <span class="math inline">\(T=50\)</span>，因此边界为 <span class="math inline">\(\frac{2}{\sqrt{50}}=0.28\)</span>。该序列所有的自相关值都落在边界之内，因此该序列为白噪声。</p><hr />]]></content>
      
      
      
        <tags>
            
            <tag> 统计 </tag>
            
            <tag> 协方差 </tag>
            
            <tag> 方差 </tag>
            
            <tag> 相关系数 </tag>
            
            <tag> 白噪声 </tag>
            
            <tag> 相关图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>『阅读笔记』iDice - Problem Identiﬁcation for Emerging Issues</title>
      <link href="/2020/04/02/ieee16_idice_reading_report/"/>
      <url>/2020/04/02/ieee16_idice_reading_report/</url>
      
        <content type="html"><![CDATA[<p>这篇 paper 由微软团队在 2016 年发表<sup id="a1"><a href="#f1">1</a></sup>。在这篇文章中，他们提出了结合了多种剪枝策略的封闭项挖掘算法，主要用于解决异常报告（emerging issues）的根因定位与根因分析等问题。</p><h2 id="背景介绍">背景介绍</h2><h3 id="问题描述和挑战">问题描述和挑战</h3><p>多维度指标的异常定位是 AIOps 领域的一个典型且有挑战的问题。在互联网服务运维中，当某个总指标（如总流；量）发生异常时，需要快速准确地定位到是哪个交叉维度的细粒度指标（如“省份=北京 &amp; 运营商=联通”的流量）的异常导致的，以便尽快做进一步的修复止损动作。由于运维中的指标维度多、每个维度的取值范围大，导致异常定位的搜索空间非常大。同时，由于这些属性通常都包括时间戳，因此诸如“挖掘频繁子集”的方法在此场景下就不太适用。</p><h3 id="有效组合">有效组合</h3><p>我们将出现问题的属性组合称为有效组合（effective combinations），因此面临的挑战是从所有可能的组合中自动且精准地识别出有效组合。整个属性组合通过子集-超集的关系形成一种格结构。每个节点表示一个属性组合，每条边表示一个子集-超集关系。</p><p>对于 X 和 Y 的两个属性组合，如果包含 X 的数据也包含在 Y 的数据中，则称 X 为 Y 的子集，Y 为 X 的超集。例如，{A, B, C} 是 {A, B} 和 {A, C} 的子集。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-03_15-40-37.png" style="zoom: 50%;" /></p><center style="font-size:14px;color:#808080;text-decoration:underline">图1. 有效组合</center><h3 id="需要达到的要求">需要达到的要求</h3><ol type="1"><li>实时性要求高。当维度较多而且各维度中属性值数目较多时，考验算法的计算效率。</li><li>元素指标之间的关系复杂。例如，当维度为位置，属性值为“北京”的 KPI 发生异常时，一般属性值为“北京移动”、“北京联通”的 KPI 也会发生异常，进而属性值为“移动”、“联通”的 KPI 也会发生异常。</li><li>结果尽可能简洁。用尽可能少的维度及其属性值的组合来表示最为全面的根因。</li></ol><h2 id="idice-算法介绍">iDice 算法介绍</h2><p>iDice 使用三种剪枝策略来减少巨大的搜索空间，分别为基于影响程度的剪枝（Impact based Pruning）、基于变化检测的剪枝（Change Detection based Pruning）和基于隔离能力的剪枝（Isolation Power based Pruning）。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-03_18-05-13.png" style="zoom:40%;" /></p><center style="font-size:14px;color:#808080;text-decoration:underline">图2. iDice 执行流程</center><h3 id="基于影响程度的剪枝">基于影响程度的剪枝</h3><p>我们仅考虑与大量问题报告相关联的属性集，而将那些问题报告不足的属性集剪掉。</p><p>用户可以通过自定义支持阈值（support threshold）<sup id="a2"><a href="#f2">2</a></sup> 来决定保留哪些属性集合。</p><p>iDice 使用基于 BFS 的封闭项集挖掘算法，在这种情况会忽略问题报告的时间戳信息。</p><p>假设我们有两组属性组合：</p><p>X = {Country=India; TenantType=Edu; DataCenter=DC6}<br />Y = {Country=USA; TenantType=Edu; DataCenter=DC1}</p><p>如果 Y 的问题发生次数低于支持阈值，而 X 的发生次数高于支持阈值，那么 Y 及其所有子集都会被剪枝，而 X 则会保留。</p><h3 id="基于变化检测的剪枝">基于变化检测的剪枝</h3><p>除了影响程度之外，我们寻找的属性组合还应该与新出现的问题相关。换句话说，我们需要找出与问题报告数量（即突发次数）显著增加相对应的属性组合。</p><p>在获得基于影响程度进行剪枝的封闭项集后，我们考虑时间戳的信息并为每个封闭项集构建对应的时间序列数据，其中每个数据点都表示问题报告的数量。这时我们需要使用变化检测算法来检测时间序列中的变化点（即发生突发事件的点）。</p><p>iDice 采用 GLR（Generalized Likelihood Ratio）<sup id="a3"><a href="#f3">3</a></sup> 作为变化检测算法。变化检测可以表述为假设检验问题。假设时间序列的值拟合分布 <span class="math inline">\(\theta_0\)</span>，而变化区域内的值符合另一个分布 <span class="math inline">\(\theta_1\)</span>。即假设 <span class="math inline">\(H_0\)</span> 对应“无变化”，假设 <span class="math inline">\(H_1\)</span> 对应“变化”。</p><p>GLR 将保持一个阈值。给定几个连续的数据点，如果它们的对数似然比之和大于阈值，则将这些连续的数据点视为变化区域，而连续数据点的第一个点被视为变化点。例如下图，从 Dec-8 到 Dec-10 的点构成变化区域，并且 Dec-8 是改变点。而对于没有任何改变点的时间序列数据，将删除相应的属性组合。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/20200405164216.png" style="zoom:50%;" /></p><center style="font-size:14px;color:#808080;text-decoration:underline">图3. 问题报告变化示例</center><p>假设我们有两组属性组合：</p><p>X = {Country=India; TenantType=Edu; DataCenter=DC6}<br />Y = {Country=UK; TenantType=Home; DataCenter=DC1}</p><p>它们对应的时间序列分别为 <span class="math inline">\(S_X\)</span> 和 <span class="math inline">\(S_Y\)</span>。如果我们发现 <span class="math inline">\(S_X\)</span> 的出现在 Dec-8（i.e. 变化点）有一个明显的变化（e.g., 从 100 上升到 300），而 <span class="math inline">\(S_Y\)</span> 并无明显的变化，则 Y 属性组合会被剪枝，而 <span class="math inline">\(S_X\)</span> 的变化点会被用在接下来的剪枝策略中。</p><h3 id="基于隔离能力的剪枝">基于隔离能力的剪枝</h3><p>有效组合应该能够将表现出变化的属性组合与其他没有变化的组合分开，为此提出了基于隔离能力的剪枝（Isolation Power based Pruning）。</p><p>隔离能力基于信息熵。另 <span class="math inline">\(S_X\)</span> 为属性组合 <span class="math inline">\(X\)</span> 对应的时间序列数据，<span class="math inline">\(X_a\)</span> 表示在 <span class="math inline">\(X\)</span> 的变化区域内，对应的时间序列 <span class="math inline">\(S_X\)</span> 的量的大小，而 <span class="math inline">\(X_b\)</span> 表示在 <span class="math inline">\(X\)</span> 的变化点之前对应的 <span class="math inline">\(S_X\)</span> 的量。<span class="math inline">\(\Omega_a\)</span> 表示 <span class="math inline">\(X\)</span> 变化区域内的整个体积，<span class="math inline">\(\Omega_b\)</span> 则表示 <span class="math inline">\(X\)</span> 的变化点之前的整个体积。<span class="math inline">\(\bar{*}\)</span> 表示相应时间序列的平均值。 <span class="math display">\[\begin{aligned}I P(X) &amp;=-\frac{1}{\overline{\Omega_{a}}+\overline{\Omega_{b}}}\left(\overline{X_{a}} \ln \frac{1}{P(a | X)}+\overline{X_{b}} \ln \frac{1}{P(b | X)}\right.\\&amp;\left.+(\overline{\Omega_{a}}-\overline{X_{a}}) \ln \frac{1}{P(a | \overline{X})}+(\overline{\Omega_{b}}-\overline{X_{b}}) \ln \frac{1}{P(b | \overline{X})}\right)\end{aligned}\]</span> 如图1所示，整个属性组合形成一个网格，网格中的每个节点都可以将数据集分为两部分：包含该属性的问题报告和不包含该属性的报告。如果属性组合是有效组合，则其所有子集节点在相同的变化区域内都应该显示出显著的增加，而其同级节点则不会。因此，一个有效的组合是可以将整个数据集精确地分为两部分的节点：是否有显著的增加。根据信息论，两个数据集（A 和 B）的总熵，在每个数据集（A 或 B）包含相同属性的样本（例如，所有样本都表现出增加，或者都表现出不增加）的情况下，会远小于样本具有不同属性的情况。 <span class="math display">\[\begin{aligned}&amp;P(a | X)=\frac{\overline{X_{a}}}{\overline{X_{b}}+\overline{\underline{X}_{a}}}, P(b | X)=\frac{\overline{X_{b}}}{\overline{X_{b}}+\overline{X_{a}}}\\&amp;P(a | \bar{X})=\frac{\overline{\Omega_{a}}-\overline{X_{a}}}{\overline{\Omega_{a}}+\overline{\Omega_{b}}-\overline{X_{b}}-\overline{X_{a}}}\\&amp;P(b | \bar{X})=\frac{\overline{\Omega_{b}}-\overline{X_{b}}}{\overline{\Omega_{a}}+\overline{\Omega_{b}}-\overline{X_{b}}-\overline{X_{a}}}\end{aligned}\]</span> 在搜索过程中，如果当前集合具有比它的直接超集和子集更高的隔离能力，则我们称当前集合是有效属性组合。在这种情况下，其所有子集将不会被搜索，从而减少搜索空间。</p><p>考虑具有三个属性组合的简单示例：</p><p>X = {Country=India; TenantType=Edu; DataCenter=DC6}<br />Y = {Country=USA; TenantType=Edu; DataCenter=DC1}</p><p>如果 Y 的发生率较低（即低于支持阈值），而 X 的发生率较高（高于支持阈值），则 Y 的所有子集将被修剪掉，而 X 将会保留。</p><h3 id="最终结果排名">最终结果排名</h3><p>根据上述剪枝的结果，我们可以获得一组有效的组合。我们根据有效组合的相对重要性对其进行排序。具体使用与 Fisher distance<sup id="a3"><a href="#f3">3</a></sup> 相似的算法进行排名。 <span class="math display">\[R = p_a \times ln \frac{p_a}{p_b}\]</span> 其中 p 由下面公式进行计算，<span class="math inline">\(V_{X_t}\)</span> 表示当前有效组合在时间段 t 的体积，<span class="math inline">\(V_t\)</span> 则表示在时间段 t 内的总体积。<span class="math inline">\(p_a\)</span> 表示在变化的时间段区间的比率，<span class="math inline">\(p_b\)</span> 则表示在检测到变化点之前的时间区间的比率。 <span class="math display">\[p = \frac{V_{X_t}}{V_t}\]</span> 从公式中可以看出，分数 R 考虑了组合的整体影响。如果两个组合具有相同的变化率（即意味着它们具有相同的趋势显著性），我们将把具有体积较大的组合排名更高。R 得分非常低的组合意义不大，可以删除。在具体的实现中，R 分数低于 cutoff 阈值将被剪掉（根据经验值设为 1.0）。最后对剩下的属性组合进行排名，并输出为有效组合。</p><h3 id="整体算法流程">整体算法流程</h3><p>算法 1 展示了上述识别有效组合的伪代码。该算法将问题报告数据（多维、时间序列数据）作为输入，并搜索与新出现的问题相关的有效组合。第 1 行的预处理包括数据清理，从而过滤出数据集中明显的噪声属性（例如所有的空值）。</p><p>对于基于 BFS 的封闭项目集挖掘过程返回的每个封闭项目集 <span class="math inline">\(p_i\)</span>，iDice 会依次执行基于影响程度的剪枝（第 6-10 行）、基于变化检测的剪枝（第 11-14 行）、基于隔离能力的剪枝（第 15-19 行）。这些步骤可以简化属性组合并减少搜索空间，从而有可能从大量属性组合中识别有效组合。第 21-26 行表示 iDice 的排名结果。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-08_15-27-36.png" style="zoom:40%;" /></p><h3 id="评测结果">评测结果</h3><p>主要与 DPMiner<sup id="a4"><a href="#f4">4</a></sup> 进行了对比，iDice 在准确率上和效率上都优于 DPMiner。更详细的内容可以去阅读原文。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-23_19-23-27.png" style="zoom:40%;" /></p><h2 id="reference">Reference</h2><p><b id="f1">[1]:</b> Lin, Qingwei, et al. iDice: problem identification for emerging issues. <em>Proceedings of the 38th International Conference on Software Engineering</em>. 2016. <a href="#a1">↩</a></p><p><b id="f2">[2]:</b> J. Han and M. Kamber. Data Mining: Concepts and Techniques. Morgan kaufmann, 2006. <a href="#a2">↩</a></p><p><b id="f3">[3]:</b> M. Basseville, I. V. Nikiforov, et al. Detection of abrupt changes: theory and application, volume 104. Prentice Hall Englewood Cliﬀs, 1993. <a href="#a3">↩</a></p><p><b id="f4">[4]:</b> J. Li, G. Liu, and L. Wong. Mining statistically important equivalence classes and delta-discriminative emerging patterns. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 430–439, 2007. <a href="#a4">↩</a></p><hr />]]></content>
      
      
      
        <tags>
            
            <tag> AIOps </tag>
            
            <tag> 根因分析 </tag>
            
            <tag> 异常检测 </tag>
            
            <tag> 论文阅读 </tag>
            
            <tag> 频繁项挖掘 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
