<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>『阅读笔记』Time-Series Anomaly Detection Service at Microsoft</title>
      <link href="/2020/04/20/kdd2019-microsoft-reading-report/"/>
      <url>/2020/04/20/kdd2019-microsoft-reading-report/</url>
      
        <content type="html"><![CDATA[<p>这篇 paper 由微软团队在 2019 年发表<sup id="a1"><a href="#f1">1</a></sup>。 他们提出了基于频谱残差（Spectual Residual, SR）和卷积神经网络（CNN）的异常检测算法，提供了一种非监督异常检测的新思路。</p><h2 id="挑战">挑战</h2><h3 id="缺乏标签">缺乏标签</h3><p>仅靠人力难以手动标记每个时间序列的异常部分。而且，时间序列的分布是不断变化的，即使是以前从未出现过的模式也要求能够识别出来，因此，只使用监督模型在工业场景中并不足够。</p><h3 id="一般化">一般化</h3><p>时间序列有几种典型的模式类别。对于异常检测服务来说，在各种时间序列模式下能够正常工作十分重要。然而，目前还没有方法能够足够的概括所有不同的模式。例如，Holt-Winters 方法在 (b) 和 (c) 模式下表现不佳，Spot 方法在 (a) 模式下也显示不够好的结果。因此，需要一种通用性更好的解决方案。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-20_16-53-57.png" style="zoom:50%;" /></p><center style="font-size:14px;color:#808080;text-decoration:underline">图1. 几种时间序列模式</center><h3 id="效率">效率</h3><p>在业务应用程序中，监控系统必须几乎实时处理数百万甚至数十亿个时间序列，特别是分钟级别的时间序列，异常检测过程需要在有限的时间内完成。因此，即使有一些具有较大时间复杂度的模型在准确率方面也很出色，但是在线上场景中很少使用。</p><h2 id="算法介绍">算法介绍</h2><p>该论文的主要目的是开发一种没有标签数据的通用高效算法。受视觉计算领域的启发，我们采用频谱残差（SR）<sup id="a2"><a href="#f2">2</a></sup>，这是一种基于快速傅里叶变换（FFT）的简单而强大的方法。SR 方法是无监督的，并且已被证明在视觉显著性检测应用中非常有效。我们认为视觉显著性检测和时间序列异常检测任务在本质上是相似的，因为异常点通常在视觉上是显著的。</p><h3 id="sr">SR</h3><p>频谱残差算法主要包括三个步骤：</p><ol type="1"><li>傅立叶变换，从而获得对数幅度频谱</li><li>计算频谱残差</li><li>傅立叶逆变换，将序列变换回空间域</li></ol><p>给定序列 <span class="math inline">\(x\)</span>，我们有如下计算式： <span class="math display">\[\begin{array}{l}A(f)=\text {Amplitude}(\widetilde{\mathfrak{F}}(\mathbf{x})) \\P(f)=\operatorname{Phrase}(\widetilde{\mathfrak{F}}(\mathbf{x})) \\L(f)=\log (A(f)) \\A L(f)=h_{q}(f) \cdot L(f) \\R(f)=L(f)-A L(f) \\S(\mathbf{x})=\left\|\mathfrak{F}^{-1}(\exp (R(f)+i P(f)))\right\|\end{array}\]</span> 其中，<span class="math inline">\(\mathfrak{F}\)</span> 和 <span class="math inline">\(\mathfrak{F}^{-1}\)</span> 分别表示傅立叶变换和傅立叶逆变换；<span class="math inline">\(x\)</span> 是维度为 <span class="math inline">\(n \times 1\)</span> 的输入序列；<span class="math inline">\(A(f)\)</span> 是序列 <span class="math inline">\(x\)</span> 的振幅频谱；<span class="math inline">\(P(f)\)</span> 是序列 <span class="math inline">\(x\)</span> 相对应的相位频谱；<span class="math inline">\(L(f)\)</span> 是 <span class="math inline">\(A(f)\)</span> 的对数表示；<span class="math inline">\(AL(f)\)</span> 是 <span class="math inline">\(L(f)\)</span> 的平均频谱，可以通过与 <span class="math inline">\(h_{q}(f)\)</span> 卷积得到，其中 <span class="math inline">\(h_{q}(f)\)</span> 是一个 <span class="math inline">\(q \times q\)</span> 的矩阵： <span class="math display">\[h_{q}(f)=\frac{1}{q^{2}}\left[\begin{array}{ccccc}1 &amp; 1 &amp; 1 &amp; \dots &amp; 1 \\1 &amp; 1 &amp; 1 &amp; \dots &amp; 1 \\\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\1 &amp; 1 &amp; 1 &amp; \dots &amp; 1\end{array}\right]\]</span> <span class="math inline">\(R(f)\)</span> 是频谱残差，即对数频谱 <span class="math inline">\(L(f)\)</span> 减去平均对数频谱 <span class="math inline">\(AL(f)\)</span>。频谱残差用作序列的压缩表示，从而使得原始序列的创新部分变得更加重要。最后，我们通过逆傅立叶变换将序列转移回空间域，<span class="math inline">\(S(x)\)</span> 称为显著图。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/20200420234023.png" style="zoom:40%;" /></p><center style="font-size:14px;color:#808080;text-decoration:underline">图2. 时间序列 SR 变换示例</center><p>图 2 显示了原始时间序列和经过 SR 处理后的显著图。如图所示，显著图中的创新点（以红色点显示）比原始输入中的创新点重要得多。基于显著图，很容易利用简单的规则正确地标注异常点。我们采用一个简单的阈值 <span class="math inline">\(\tau\)</span> 来进行异常点判断，如下公式所示： <span class="math display">\[O\left(x_{i}\right)=\left\{\begin{array}{ll}1, &amp; \text { if }\left.\frac{S\left(x_{i}\right)-\overline{S\left(x_{i}\right)}}{\overline{S\left(x_{i}\right)}}\right)&gt;\tau \\0, &amp; \text { otherwise }\end{array}\right.\]</span> 其中 <span class="math inline">\(x(i)\)</span> 代表序列中任意点，<span class="math inline">\(S(x_i)\)</span> 是显著图中的对应点，<span class="math inline">\(\overline{S(x_i)}\)</span> 是 <span class="math inline">\(S(x_i)\)</span> 之前 <span class="math inline">\(z\)</span> 个点的局部平均值。</p><p>在实际操作中，FFT 是在序列的滑动窗口内进行的。进一步来说，我们期望该算法能够发现具有低延迟的异常点。也就是说，给定一个序列 <span class="math inline">\(x_1, x_2, \cdots, x_n\)</span>，其中 <span class="math inline">\(x_n\)</span> 是序列中最近的点，因此我们想尽快判断 <span class="math inline">\(x_n\)</span> 是否为一个异常点。然而，当目标点位于滑动窗口的中心时，SR 算法的效果最好。因此，在将序列输入 SR 模型之前，我们会在 <span class="math inline">\(x_n\)</span> 之后添加好几个估计点。估计点 <span class="math inline">\(x_{n+1}\)</span> 由下式计算： <span class="math display">\[\begin{array}{c}\bar{g}=\frac{1}{m} \sum_{i=1}^{m} g\left(x_{n}, x_{n-i}\right) \\x_{n+1}=x_{n-m+1}+\bar{g} \cdot m\end{array}\]</span> 其中，<span class="math inline">\(g(x_i, x_j)\)</span> 表示点 <span class="math inline">\(x_i\)</span> 与 <span class="math inline">\(x_j\)</span> 之间直线的梯度，<span class="math inline">\(\bar{g}\)</span> 表示之前各点的平均梯度，<span class="math inline">\(m\)</span> 是之前点的数量，在实现中我们将 <span class="math inline">\(m\)</span> 设置为 5。我们发现，第一个估计点起着决定性的作用，因此，我们只将 <span class="math inline">\(x_{n+1}\)</span> 复制 <span class="math inline">\(\kappa\)</span> 次，并将这些点添加到序列的尾部。</p><p>综上所述，SR 算法仅包含几个超参数，即滑动窗口大小 <span class="math inline">\(\omega\)</span>，估计点的数量 <span class="math inline">\(\kappa\)</span> 和异常检测阈值 <span class="math inline">\(\tau\)</span>。我们根据经验来设置它们，并在试验中显示其鲁棒性。因此，SR 算法是在线异常检测服务的不错选择。</p><h3 id="sr-cnn">SR-CNN</h3><p>原始 SR 方法利用显著图上的单个阈值来检测异常点，但其规则过于简单，因此需要寻求更复杂的决策规则。我们的理念是通过精心设计的合成数据，训练出异常检测的判别模型。合成数据可以通过向显著图集合中插入不包含在评估数据集中的异常点来实现。因此，注入点会被标注为异常，而其他标记为正常。具体来说，我们在时间序列中随机选择几个点，计算插入值从而替换原始点，并获得其显著图。异常点的值通过以下方式计算： <span class="math display">\[x=(\bar{x}+m e a n)(1+v a r) \cdot r+x\]</span> 其中 <span class="math inline">\(\bar{x}\)</span> 是先前点的本地平均；mean 和 var 是当前滑动窗口内所有点的均值和方差；<span class="math inline">\(r \sim \mathcal{N}(0,1)\)</span>，<span class="math inline">\(r\)</span> 是正态分布的随机采样。</p><p>我们选择 CNN 作为判别模型的架构。CNN 是常用于显著性检测的监督模型。但是，由于场景中通常没有足够的标记数据，因此我们使用基于显著图的 CNN，而不是原始的输入，这使得异常标注的问题更加容易。在实践中，我们使用具有合成异常点的时间序列作为训练集，这样的优点是检测器适应时间序列分布的变化，而无需手动标记的数据。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-21_12-17-56.png" style="zoom:50%;" /></p><center style="font-size:14px;color:#808080;text-decoration:underline">图3. SR-CNN 结构</center><p>SR-CNN 的结构如图 3 所示。该网络由两个一维卷积层（filter 大小等于滑动窗口大小 <span class="math inline">\(\omega\)</span>）和两个全连接层组成。第一个卷积层 channel 大小等于 <span class="math inline">\(\omega\)</span>，第二个卷积层 channel 大小增加一倍。在 Sigmoid 输出前，堆叠了两个全连接层。使用交叉熵作为损失函数，SGD 作为优化器。</p><h2 id="实验评估">实验评估</h2><p>使用三个数据集进行评测，<a href="http://iops.ai/competition_detail/?competition_id=5&amp;flag=1." target="_blank" rel="noopener">AIOPS 竞赛的 KPI 数据集</a>、<a href="https://yahooresearch.tumblr.com/post/114590420346/" target="_blank" rel="noopener">Yahoo 的公共数据集</a>和 Microsoft 的内部数据集。KPI 数据集的时间间隔点为 1 分钟或 5 分钟；Yahoo 数据集中一部分是合成的，另一部分是实际的流量，时间间隔为 1 小时；Microsoft 数据集的时间间隔为 1 天。</p><p>FFT<sup id="a3"><a href="#f3">3</a></sup>（快速傅立叶变换）、Twitter-AD<sup id="a4"><a href="#f4">4</a></sup> 和 Luminol<sup id="a5"><a href="#f5">5</a></sup>（LinkedIn 的异常检测）不需要其他数据即可启动（冷启动）；另一方面，SPOT、DSPOT<sup id="a6"><a href="#f6">6</a></sup> 和 DONUT<sup id="a7"><a href="#f7">7</a></sup> 则需要其他数据来训练模型。因此根据时间顺序将每个时间序列的点分为两半，上半部分由于训练、下半部分由于测试评估。</p><p>我们将 SR 和 SR-CNN 的超参数设置为如下：<span class="math inline">\(h_q(f)\)</span> 中的 <span class="math inline">\(q\)</span> 设为 3，先前点的数量 <span class="math inline">\(z\)</span>（由于计算本地平均值）设为 21、阈值 <span class="math inline">\(\tau\)</span> 设为 3，估计点的数量 <span class="math inline">\(\kappa\)</span> 设为 5。滑动窗口大小 <span class="math inline">\(\omega\)</span> 在 KPI 上设为 1440，在 Yahoo 上设为 64，在 Microsoft 上设为 30。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-21_13-33-07.png" /></p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-21_13-48-27.png" style="zoom:40%;" /></p><p>从表格中可以看到，SR 和 SR-CNN 的效果基本上都是最好的。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-21_13-50-32.png" style="zoom:40%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-21_13-51-38.png" style="zoom:50%;" /></p><p>另外，使用 SR+DNN 在监督训练中，也比单纯的 DNN 表现效果要更好一些。</p><h2 id="reference">Reference</h2><p><b id="f1">[1]:</b> Ren, Hansheng, et al. "Time-Series Anomaly Detection Service at Microsoft." <em>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 2019. <a href="#a1">↩︎</a></p><p><b id="f2">[2]:</b> Hou, Xiaodi, and Liqing Zhang. "Saliency detection: A spectral residual approach." <em>2007 IEEE Conference on computer vision and pattern recognition</em>. Ieee, 2007. <a href="#a2">↩︎</a></p><p><b id="f3">[3]: </b> Faraz Rasheed, Peter Peng, Reda Alhajj, and Jon Rokne. 2009. Fourier trans- form based spatial outlier mining. In International Conference on Intelligent Data Engineering and Automated Learning. Springer, 317–324. <a href="#a3">↩︎</a></p><p><b id="f4">[4]:</b> Owen Vallis, Jordan Hochenbaum, and Arun Kejariwal. 2014. A Novel Technique for Long-Term Anomaly Detection in the Cloud. In 6th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 14). USENIX Association, Philadelphia, PA. <a href="#a4">↩︎</a></p><p><b id="f5">[5]:</b> https://github.com/linkedin/luminol. <a href="#a5">↩︎</a></p><p><b id="f6">[6]:</b> Alban Siffer, Pierre-Alain Fouque, Alexandre Termier, and Christine Largouet. 2017. Anomaly detection in streams with extreme value theory. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 1067–1075. <a href="#a6">↩︎</a></p><p><b id="f7">[7]:</b> Haowen Xu, Wenxiao Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li, Ying Liu, Youjian Zhao, Dan Pei, Yang Feng, et al. 2018. Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications. In Proceedings of the 2018 World Wide Web Conference on World Wide Web. Inter- national World Wide Web Conferences Steering Committee, 187–196. <a href="#a7">↩︎</a></p><hr />]]></content>
      
      
      
        <tags>
            
            <tag> AIOps </tag>
            
            <tag> 异常检测 </tag>
            
            <tag> 论文阅读 </tag>
            
            <tag> 卷积神经网络 </tag>
            
            <tag> CNN </tag>
            
            <tag> Spectral Residual </tag>
            
            <tag> 频谱残差 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>协方差 and 相关系数</title>
      <link href="/2020/04/11/statistic_intro/"/>
      <url>/2020/04/11/statistic_intro/</url>
      
        <content type="html"><![CDATA[<h2 id="期望值">期望值</h2><p>在概率论和统计学中，一个离散性随机变量的期望值，是试验中每次可能的结果乘以其概率结果的总和。而在实际的应用或案例中，我们通常遇到的情况都是离散的。因此这里只简单介绍一下此种情况的期望值如何计算。</p><h3 id="定义">定义</h3><p>若 <span class="math inline">\(X\)</span> 是离散的随机变量，输出值为 <span class="math inline">\(x_1, x_2, \cdots\)</span>，与输出值相应的概率为 <span class="math inline">\(p_1, p_2, \cdots\)</span>（概率和为1），则 <span class="math inline">\(X\)</span> 的期望值如下： <span class="math display">\[E(X) = \sum_i{p_ix_i}\]</span> 当样本发生概率都相同时（平均分布），期望值又等于均值 <span class="math inline">\(E(X) = \bar{x}\)</span>。</p><h3 id="性质">性质</h3><ul><li><p>期望值是线性函数</p><p><span class="math inline">\(E(aX+bY)=aE(X)+bE(Y)\)</span></p></li><li><p>当随机变量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 的协方差为 0 时（又称它们不相关），下面等式成立。（特别地，当 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 独立时，协方差也为 0）</p><p><span class="math inline">\(E(XY)=E(X) \cdot E(Y)\)</span></p></li></ul><h2 id="协方差">协方差</h2><p>协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。通俗来说，就是两个变量在变化过程中的变化趋势，是同方向变化还是反方向变化？同向或反向程度如何？</p><p>若两个变量是同向变化的，这时协方差就是正的；若两个变量反向变化，这时协方差就是负的。协方差数值越大，表示两个变量的同向程度也越大。</p><h3 id="定义-1">定义</h3><p>若随机变量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 的期望值分别是 <span class="math inline">\(E(X)=\mu, E(Y)=\nu\)</span>，则 <span class="math inline">\(X\)</span> 与 <span class="math inline">\(Y\)</span> 之间的协方差为： <span class="math display">\[Cov(X, Y) = E[(X-\mu)(Y-\nu)] = E(XY) - \mu\nu\]</span> 该公式简单来说就是，两个变量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 的协方差为，「每个时刻的 <span class="math inline">\(X\)</span> 与其期望之差」乘以「对应时刻的 <span class="math inline">\(Y\)</span> 与其期望之差」，最后再对得到的乘积求和并计算其期望值。</p><p>若 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 是统计独立的，那么二者的协方差为 0，这是因为 <span class="math display">\[E(XY) = E(X) \cdot E(Y) = \mu\nu\]</span> 但是反过来并不成立，即若 <span class="math inline">\(X\)</span> 与 <span class="math inline">\(Y\)</span> 的协方差为 0，二者并不一定是统计独立的，但是二者之间一定是不相关的。</p><p>另外，对于 N 个相等概率值的平均分布， <span class="math display">\[Cov(X, Y) = \frac{1}{N} \sum_i^N (x_i-\bar{x})(y_i-\bar{y})\]</span></p><h2 id="方差">方差</h2><p>方差（Variance）是协方差的特殊情况，即两个变量是相同的。在概率论和统计学中，一个随机变量的方差描述的是它的离散程度，也就是该变量与其期望值的距离。方差越大，代表大部分数值与其期望之间的差异较大。</p><h3 id="定义-2">定义</h3><p>若随机变量 <span class="math inline">\(X\)</span> 的期望值 <span class="math inline">\(E(X)=\mu\)</span>，则 <span class="math inline">\(X\)</span> 的方差为： <span class="math display">\[\begin{aligned}Var(X) &amp;= E[(X-\mu)^2] = Cov(X, X)\end{aligned}\]</span> 将上式展开可得： <span class="math display">\[Var(X) = E[X^2-2X \cdot E(X)+[E(X)]^2]=E(X^2)-2E(X) \cdot E(X) + [E(X)]^2 = E(X^2)-[E(X)]^2\]</span> 即 <span class="math inline">\(X\)</span> 的方差为「其平方的期望」减去「其期望的平方」。</p><p>若 <span class="math inline">\(X\)</span> 是离散的随机变量，输出值为 <span class="math inline">\(x_1, x_2, \cdots\)</span>，与输出值相应的概率为 <span class="math inline">\(p_1, p_2, \cdots\)</span>（概率和为1），则 <span class="math inline">\(X\)</span> 的方差如下： <span class="math display">\[\begin{aligned}E(X) &amp;= \sum_i{p_ix_i} = \mu\\Var(X) &amp;= \sum_i p_i \cdot (x_i-\mu)^2 = \sum_i(p_i \cdot {x_i}^2) - \mu^2\end{aligned}\]</span> 而对于 N 个相等概率值的平均分布， <span class="math display">\[Var(X) = \sigma^2 = \frac{1}{N} \sum_i^N(x_i-\bar{x})^2 = \frac{1}{N}\sum_i^N{x_i}^2 - {\bar{x}}^2\]</span></p><h3 id="标准差">标准差</h3><p>我们通常用 <span class="math inline">\(\sigma^2\)</span> 来表示方差，而 <span class="math inline">\(\sigma\)</span> 则表示标准差（Standard Deviation）。</p><p>标准差与方差一样用来表示组内数据间的离散程度，但是相比于方差，标准差用来表示离散程度的数字与样本数据点的数量级和单位一致，更容易理解和后续的分析计算。</p><p>另外，在样本数据大致符合正态分布的情况下，标准差具有方便估算的特性：68.3% 的数据点落在平均值前后 1 个标准差的范围内；95.4% 的数据点落在平均值前后 2 个标准差的范围内；而 99.7% 的数据点将会落在平均值前后 3 个标准差的范围内。因此可以使用 <span class="math inline">\(3\sigma\)</span> 定律排除掉异常的数据点。</p><h2 id="相关系数">相关系数</h2><h3 id="定义-3">定义</h3><p>两个变量之间的皮尔逊相关系数（Pearson Correlation Coeffocient）定义为两个变量之间的协方差和标准差的商，用于度量变量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 之间的线性相关关系，取值范围为 [-1, 1]，其中正数表示正线性相关，负数表示负线性相关，而 0 表示非线性相关。 <span class="math display">\[\rho_{X, Y} = \frac{Cov(X, Y)}{\sigma_X \sigma_Y}\]</span> 因此，相关系数也可以看作成一种协方差：一种剔除了两个变量的量纲影响、标准化后的特殊协方差。它消除了两个变量间变化幅度的影响，更能反应出两个变量每单位变化时的相似程度。</p><p>而对于样本的相关系数，常用 <span class="math inline">\(r\)</span> 来表示： <span class="math display">\[r = \frac{\sum_{i=1}^N (x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^N(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^N(y_i-\bar{y})^2}}\]</span> 下图是几组 (x, y) 的点集，以及各个点集中 x 和 y 之间的相关系数。我们可以发现相关系数反映的是变量之间的线性关系和相关性的方向（第一排），而不是相关性的斜率（中间），也不是各种非线性关系（第三排）。请注意：中间的图中斜率为 0，但相关系数是没有意义的，因为此时变量 Y 是 0。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-12_20-40-17.png" style="zoom:20%;" /></p><center style="font-size:16px;color:#808080;text-decoration:underline">图1. 来自维基百科</center><h2 id="自相关系数">自相关系数</h2><p>正如相关系数可以衡量两个变量之间的线性相关关系一样，自相关系数（Autocorrelation）可以测量时间序列 <strong>滞后值</strong> 之间的线性关系。</p><h3 id="定义-4">定义</h3><p>我们用 <span class="math inline">\(r_k\)</span> 来表示自相关系数，例如 <span class="math inline">\(r_1\)</span> 衡量的是 <span class="math inline">\(y_t\)</span> 和 <span class="math inline">\(y_{t-1}\)</span> 之间的关系， <span class="math inline">\(r_2\)</span> 衡量的是 <span class="math inline">\(y_t\)</span> 和 <span class="math inline">\(y_{t-2}\)</span> 之间的关系。其中 <span class="math inline">\(T\)</span> 是时间序列的长度 <span class="math display">\[r_k = \frac{\sum_{t=k+1}^T (y_t-\bar{y})(y_{t-k}-\bar{y})}{\sqrt{\sum_{t=1}^T(y_t-\bar{y})^2}}\]</span></p><p>自相关系数值越高，代表这两个时间点具有更高的线性相关性。</p><h3 id="自相关图">自相关图</h3><p>通过将自相关系数 <span class="math inline">\(r_k\)</span> 绘制成图，即可得到自相关函数（Autocorrelation），也称作相关图（ACF）。</p><p><img src="https://otexts.com/fppcn/fpp_files/figure-html/aelec-1.png" style="zoom:80%;"   ></p><p>上图表示 1980-1995 年间澳大利亚月度用电量，可以看出该图具有向上的趋势，以及明显的季节性。下图则是其对应的 ACF 图。</p><p><img src="https://otexts.com/fppcn/fpp_files/figure-html/acfelec-1.png" style="zoom:80%;" ></p><p>当数据具有趋势性时，短期滞后的自相关值较大，因为观测点附近的值波动不会很大（也就是说，离观测点近的数据点会和观测点的数值相近）。这时候的时间序列的 ACF 一般是正值，随着滞后阶数的增加而缓慢下降。</p><p>当数据具有季节性时，自相关值在滞后阶数与季节周期相同（或其倍数）时较大。可以看到图中，每年会有一个高峰和低估，因此在滞后阶数为 12 的倍数时，其自相关值会呈现「圆齿状」。</p><h2 id="白噪声">白噪声</h2><p>白噪声是一个对所有时间，其自相关系数为零的随机过程。</p><p><img src="https://otexts.com/fppcn/fpp_files/figure-html/wnoise-1.png" style="zoom: 80%;" ></p><p><img src="https://otexts.com/fppcn/fpp_files/figure-html/wnoiseacf-1.png" style="zoom:80%;" ></p><p>对于白噪声而言，我们期望它的自相关值接近零。但是由于随机扰动的存在，自相关值并不会精确地等于零。对于一个长度为 <span class="math inline">\(T\)</span> 的白噪声序列而言，我们期望在 0.95 的置信度下，它的自相关值处于 <span class="math inline">\(\frac{2}{\sqrt{T}}\)</span> 之间。我们可以很容易的画出 ACF 的边界值（图中蓝色虚线）。如果一个序列中有较多自相关值处于边界之外，那么该序列很可能不是白噪声序列。</p><p>在上例中，序列长度 <span class="math inline">\(T=50\)</span>，因此边界为 <span class="math inline">\(\frac{2}{\sqrt{50}}=0.28\)</span>。该序列所有的自相关值都落在边界之内，因此该序列为白噪声。</p><hr />]]></content>
      
      
      
        <tags>
            
            <tag> 统计 </tag>
            
            <tag> 协方差 </tag>
            
            <tag> 方差 </tag>
            
            <tag> 相关系数 </tag>
            
            <tag> 白噪声 </tag>
            
            <tag> 相关图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>『阅读笔记』 iDice - Problem Identiﬁcation for Emerging Issues</title>
      <link href="/2020/04/02/idice_reading_report/"/>
      <url>/2020/04/02/idice_reading_report/</url>
      
        <content type="html"><![CDATA[<p>这篇 paper 由微软团队在 2016 年发表<sup id="a1"><a href="#f1">1</a></sup>。在这篇文章中，他们提出了结合了多种剪枝策略的封闭项挖掘算法，主要用于解决异常报告（emerging issues）的根因定位与根因分析等问题。</p><h2 id="背景介绍">背景介绍</h2><h3 id="问题描述和挑战">问题描述和挑战</h3><p>多维度指标的异常定位是 AIOps 领域的一个典型且有挑战的问题。在互联网服务运维中，当某个总指标（如总流；量）发生异常时，需要快速准确地定位到是哪个交叉维度的细粒度指标（如“省份=北京 &amp; 运营商=联通”的流量）的异常导致的，以便尽快做进一步的修复止损动作。由于运维中的指标维度多、每个维度的取值范围大，导致异常定位的搜索空间非常大。同时，由于这些属性通常都包括时间戳，因此诸如“挖掘频繁子集”的方法在此场景下就不太适用。</p><h3 id="有效组合">有效组合</h3><p>我们将出现问题的属性组合称为有效组合（effective combinations），因此面临的挑战是从所有可能的组合中自动且精准地识别出有效组合。整个属性组合通过子集-超集的关系形成一种格结构。每个节点表示一个属性组合，每条边表示一个子集-超集关系。</p><p>对于 X 和 Y 的两个属性组合，如果包含 X 的数据也包含在 Y 的数据中，则称 X 为 Y 的子集，Y 为 X 的超集。例如，{A, B, C} 是 {A, B} 和 {A, C} 的子集。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-03_15-40-37.png" style="zoom: 50%;" /></p><center style="font-size:14px;color:#808080;text-decoration:underline">图1. 有效组合</center><h3 id="需要达到的要求">需要达到的要求</h3><ol type="1"><li>实时性要求高。当维度较多而且各维度中属性值数目较多时，考验算法的计算效率。</li><li>元素指标之间的关系复杂。例如，当维度为位置，属性值为“北京”的 KPI 发生异常时，一般属性值为“北京移动”、“北京联通”的 KPI 也会发生异常，进而属性值为“移动”、“联通”的 KPI 也会发生异常。</li><li>结果尽可能简洁。用尽可能少的维度及其属性值的组合来表示最为全面的根因。</li></ol><h2 id="idice-算法介绍">iDice 算法介绍</h2><p>iDice 使用三种剪枝策略来减少巨大的搜索空间，分别为基于影响程度的剪枝（Impact based Pruning）、基于变化检测的剪枝（Change Detection based Pruning）和基于隔离能力的剪枝（Isolation Power based Pruning）。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-03_18-05-13.png" style="zoom:40%;" /></p><center style="font-size:14px;color:#808080;text-decoration:underline">图2. iDice 执行流程</center><h3 id="基于影响程度的剪枝">基于影响程度的剪枝</h3><p>我们仅考虑与大量问题报告相关联的属性集，而将那些问题报告不足的属性集剪掉。</p><p>用户可以通过自定义支持阈值（support threshold）<sup id="a2"><a href="#f2">2</a></sup> 来决定保留哪些属性集合。</p><p>iDice 使用基于 BFS 的封闭项集挖掘算法，在这种情况会忽略问题报告的时间戳信息。</p><p>假设我们有两组属性组合：</p><p>X = {Country=India; TenantType=Edu; DataCenter=DC6}<br />Y = {Country=USA; TenantType=Edu; DataCenter=DC1}</p><p>如果 Y 的问题发生次数低于支持阈值，而 X 的发生次数高于支持阈值，那么 Y 及其所有子集都会被剪枝，而 X 则会保留。</p><h3 id="基于变化检测的剪枝">基于变化检测的剪枝</h3><p>除了影响程度之外，我们寻找的属性组合还应该与新出现的问题相关。换句话说，我们需要找出与问题报告数量（即突发次数）显著增加相对应的属性组合。</p><p>在获得基于影响程度进行剪枝的封闭项集后，我们考虑时间戳的信息并为每个封闭项集构建对应的时间序列数据，其中每个数据点都表示问题报告的数量。这时我们需要使用变化检测算法来检测时间序列中的变化点（即发生突发事件的点）。</p><p>iDice 采用 GLR（Generalized Likelihood Ratio）<sup id="a3"><a href="#f3">3</a></sup> 作为变化检测算法。变化检测可以表述为假设检验问题。假设时间序列的值拟合分布 <span class="math inline">\(\theta_0\)</span>，而变化区域内的值符合另一个分布 <span class="math inline">\(\theta_1\)</span>。即假设 <span class="math inline">\(H_0\)</span> 对应“无变化”，假设 <span class="math inline">\(H_1\)</span> 对应“变化”。</p><p>GLR 将保持一个阈值。给定几个连续的数据点，如果它们的对数似然比之和大于阈值，则将这些连续的数据点视为变化区域，而连续数据点的第一个点被视为变化点。例如下图，从 Dec-8 到 Dec-10 的点构成变化区域，并且 Dec-8 是改变点。而对于没有任何改变点的时间序列数据，将删除相应的属性组合。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/20200405164216.png" style="zoom:50%;" /></p><center style="font-size:14px;color:#808080;text-decoration:underline">图3. 问题报告变化示例</center><p>假设我们有两组属性组合：</p><p>X = {Country=India; TenantType=Edu; DataCenter=DC6}<br />Y = {Country=UK; TenantType=Home; DataCenter=DC1}</p><p>它们对应的时间序列分别为 <span class="math inline">\(S_X\)</span> 和 <span class="math inline">\(S_Y\)</span>。如果我们发现 <span class="math inline">\(S_X\)</span> 的出现在 Dec-8（i.e. 变化点）有一个明显的变化（e.g., 从 100 上升到 300），而 <span class="math inline">\(S_Y\)</span> 并无明显的变化，则 Y 属性组合会被剪枝，而 <span class="math inline">\(S_X\)</span> 的变化点会被用在接下来的剪枝策略中。</p><h3 id="基于隔离能力的剪枝">基于隔离能力的剪枝</h3><p>有效组合应该能够将表现出变化的属性组合与其他没有变化的组合分开，为此提出了基于隔离能力的剪枝（Isolation Power based Pruning）。</p><p>隔离能力基于信息熵。另 <span class="math inline">\(S_X\)</span> 为属性组合 <span class="math inline">\(X\)</span> 对应的时间序列数据，<span class="math inline">\(X_a\)</span> 表示在 <span class="math inline">\(X\)</span> 的变化区域内，对应的时间序列 <span class="math inline">\(S_X\)</span> 的量的大小，而 <span class="math inline">\(X_b\)</span> 表示在 <span class="math inline">\(X\)</span> 的变化点之前对应的 <span class="math inline">\(S_X\)</span> 的量。<span class="math inline">\(\Omega_a\)</span> 表示 <span class="math inline">\(X\)</span> 变化区域内的整个体积，<span class="math inline">\(\Omega_b\)</span> 则表示 <span class="math inline">\(X\)</span> 的变化点之前的整个体积。<span class="math inline">\(\bar{*}\)</span> 表示相应时间序列的平均值。 <span class="math display">\[\begin{aligned}I P(X) &amp;=-\frac{1}{\overline{\Omega_{a}}+\overline{\Omega_{b}}}\left(\overline{X_{a}} \ln \frac{1}{P(a | X)}+\overline{X_{b}} \ln \frac{1}{P(b | X)}\right.\\&amp;\left.+(\overline{\Omega_{a}}-\overline{X_{a}}) \ln \frac{1}{P(a | \overline{X})}+(\overline{\Omega_{b}}-\overline{X_{b}}) \ln \frac{1}{P(b | \overline{X})}\right)\end{aligned}\]</span> 如图1所示，整个属性组合形成一个网格，网格中的每个节点都可以将数据集分为两部分：包含该属性的问题报告和不包含该属性的报告。如果属性组合是有效组合，则其所有子集节点在相同的变化区域内都应该显示出显著的增加，而其同级节点则不会。因此，一个有效的组合是可以将整个数据集精确地分为两部分的节点：是否有显著的增加。根据信息论，两个数据集（A 和 B）的总熵，在每个数据集（A 或 B）包含相同属性的样本（例如，所有样本都表现出增加，或者都表现出不增加）的情况下，会远小于样本具有不同属性的情况。 <span class="math display">\[\begin{aligned}&amp;P(a | X)=\frac{\overline{X_{a}}}{\overline{X_{b}}+\overline{\underline{X}_{a}}}, P(b | X)=\frac{\overline{X_{b}}}{\overline{X_{b}}+\overline{X_{a}}}\\&amp;P(a | \bar{X})=\frac{\overline{\Omega_{a}}-\overline{X_{a}}}{\overline{\Omega_{a}}+\overline{\Omega_{b}}-\overline{X_{b}}-\overline{X_{a}}}\\&amp;P(b | \bar{X})=\frac{\overline{\Omega_{b}}-\overline{X_{b}}}{\overline{\Omega_{a}}+\overline{\Omega_{b}}-\overline{X_{b}}-\overline{X_{a}}}\end{aligned}\]</span> 在搜索过程中，如果当前集合具有比它的直接超集和子集更高的隔离能力，则我们称当前集合是有效属性组合。在这种情况下，其所有子集将不会被搜索，从而减少搜索空间。</p><p>考虑具有三个属性组合的简单示例：</p><p>X = {Country=India; TenantType=Edu; DataCenter=DC6}<br />Y = {Country=USA; TenantType=Edu; DataCenter=DC1}</p><p>如果 Y 的发生率较低（即低于支持阈值），而 X 的发生率较高（高于支持阈值），则 Y 的所有子集将被修剪掉，而 X 将会保留。</p><h3 id="最终结果排名">最终结果排名</h3><p>根据上述剪枝的结果，我们可以获得一组有效的组合。我们根据有效组合的相对重要性对其进行排序。具体使用与 Fisher distance<sup id="a3"><a href="#f3">3</a></sup> 相似的算法进行排名。 <span class="math display">\[R = p_a \times ln \frac{p_a}{p_b}\]</span> 其中 p 由下面公式进行计算，<span class="math inline">\(V_{X_t}\)</span> 表示当前有效组合在时间段 t 的体积，<span class="math inline">\(V_t\)</span> 则表示在时间段 t 内的总体积。<span class="math inline">\(p_a\)</span> 表示在变化的时间段区间的比率，<span class="math inline">\(p_b\)</span> 则表示在检测到变化点之前的时间区间的比率。 <span class="math display">\[p = \frac{V_{X_t}}{V_t}\]</span> 从公式中可以看出，分数 R 考虑了组合的整体影响。如果两个组合具有相同的变化率（即意味着它们具有相同的趋势显著性），我们将把具有体积较大的组合排名更高。R 得分非常低的组合意义不大，可以删除。在具体的实现中，R 分数低于 cutoff 阈值将被剪掉（根据经验值设为 1.0）。最后对剩下的属性组合进行排名，并输出为有效组合。</p><h3 id="整体算法流程">整体算法流程</h3><p>算法 1 展示了上述识别有效组合的伪代码。该算法将问题报告数据（多维、时间序列数据）作为输入，并搜索与新出现的问题相关的有效组合。第 1 行的预处理包括数据清理，从而过滤出数据集中明显的噪声属性（例如所有的空值）。</p><p>对于基于 BFS 的封闭项目集挖掘过程返回的每个封闭项目集 <span class="math inline">\(p_i\)</span>，iDice 会依次执行基于影响程度的剪枝（第 6-10 行）、基于变化检测的剪枝（第 11-14 行）、基于隔离能力的剪枝（第 15-19 行）。这些步骤可以简化属性组合并减少搜索空间，从而有可能从大量属性组合中识别有效组合。第 21-26 行表示 iDice 的排名结果。</p><p><img src="https://cdn.jsdelivr.net/gh/davidlight2018/figure_bed/img/2020-04-08_15-27-36.png" style="zoom:40%;" /></p><h3 id="评测结果">评测结果</h3><p>评测结果就不在这里赘述了，毕竟每种数据集的各种参数都不相同，有兴趣的去看看原文。</p><h2 id="reference">Reference</h2><p><b id="f1">[1]:</b> Lin, Qingwei, et al. iDice: problem identification for emerging issues. <em>Proceedings of the 38th International Conference on Software Engineering</em>. 2016. <a href="#a1">↩︎</a></p><p><b id="f2">[2]:</b> J. Han and M. Kamber. Data Mining: Concepts and Techniques. Morgan kaufmann, 2006. <a href="#a2">↩︎</a></p><p><b id="f3">[3]:</b> M. Basseville, I. V. Nikiforov, et al. Detection of abrupt changes: theory and application, volume 104. Prentice Hall Englewood Cliﬀs, 1993. <a href="#a3">↩︎</a></p><hr />]]></content>
      
      
      
        <tags>
            
            <tag> AIOps </tag>
            
            <tag> 根因分析 </tag>
            
            <tag> 异常检测 </tag>
            
            <tag> 论文阅读 </tag>
            
            <tag> 频繁项挖掘 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
